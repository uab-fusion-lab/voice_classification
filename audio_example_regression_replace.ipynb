{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:23:14.057097900Z",
     "start_time": "2024-07-31T14:23:05.490176900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "[('020', 78.86),\n ('022', 83.72),\n ('023', 89.58),\n ('024', 96.51),\n ('025', 67.07),\n ('026', 73.86),\n ('029', 70.62),\n ('030', 89.22),\n ('031', 75.68),\n ('032', 78.21),\n ('033', 71.77),\n ('034', 60.95),\n ('035', 81.99),\n ('036', 80.05),\n ('037', 90.6),\n ('038', 82.59),\n ('039', 81.06),\n ('040', 81.5),\n ('041', 44.05),\n ('042', 77.16),\n ('043', 75.69),\n ('044', 82.62),\n ('045', 66.13),\n ('046', 76.57),\n ('047', 65.16),\n ('048', 63.78),\n ('049', 58.63),\n ('051', 68.89),\n ('052', 83.25),\n ('053', 85.91),\n ('054', 73.39),\n ('057', 55.0),\n ('058', 68.51),\n ('061', 88.75),\n ('063', 67.59),\n ('064', 61.47),\n ('065', 75.97),\n ('066', 69.39),\n ('067', 69.55),\n ('068', 83.77),\n ('069', 69.4),\n ('070', 33.99),\n ('073', 57.3),\n ('074', 79.12),\n ('075', 80.09),\n ('076', 75.58),\n ('078', 46.58),\n ('079', 76.7),\n ('080', 81.79),\n ('081', 87.53),\n ('082', 80.36),\n ('083', 91.9),\n ('084', 76.89),\n ('085', 79.3),\n ('088', 84.25),\n ('091', 63.78),\n ('093', 80.25),\n ('094', 87.95),\n ('095', 54.19),\n ('098', 47.47),\n ('099', 74.39),\n ('102', 79.16),\n ('104', 85.01),\n ('105', 37.17),\n ('107', 72.23),\n ('108', 72.98),\n ('109', 79.51),\n ('110', 80.98),\n ('111', 80.73),\n ('112', 81.23),\n ('113', 88.53),\n ('114', 79.3),\n ('115', 67.47),\n ('116', 79.87),\n ('117', 83.65),\n ('118', 87.59),\n ('119', 70.77),\n ('120', 43.94),\n ('121', 75.96),\n ('122', 88.79),\n ('123', 84.12),\n ('124', 76.72),\n ('125', 38.75),\n ('126', 79.01),\n ('127', 84.1),\n ('128', 74.88),\n ('129', 83.96),\n ('130', 91.31),\n ('131', 86.29),\n ('132', 83.71),\n ('133', 71.04),\n ('134', 84.7),\n ('135', 83.86),\n ('136', 84.66),\n ('137', 79.44),\n ('138', 83.32),\n ('139', 74.16),\n ('140', 75.72),\n ('141', 76.34),\n ('142', 74.05),\n ('143', 71.86),\n ('145', 75.5),\n ('147', 82.16),\n ('149', 84.78),\n ('150', 84.44),\n ('151', 62.84),\n ('152', 80.2),\n ('153', 67.72),\n ('154', 69.11),\n ('155', 86.15),\n ('156', 76.89),\n ('157', 28.3),\n ('158', 75.39),\n ('159', 54.08),\n ('161', 71.27),\n ('162', 78.81),\n ('163', 92.22),\n ('164', 73.08),\n ('165', 94.61),\n ('166', 81.5),\n ('167', 76.91),\n ('168', 30.88),\n ('170', 79.58),\n ('171', 71.88),\n ('172', 73.29),\n ('173', 79.33),\n ('174', 77.98),\n ('175', 51.3),\n ('176', 81.99),\n ('177', 59.0),\n ('178', 39.1),\n ('179', 62.9),\n ('180', 46.51),\n ('181', 74.79),\n ('182', 77.93),\n ('183', 81.45),\n ('184', 80.3),\n ('185', 93.33),\n ('186', 79.51),\n ('187', 78.15),\n ('188', 79.05),\n ('189', 81.27),\n ('190', 84.72),\n ('192', 59.59),\n ('193', 69.87),\n ('194', 62.54),\n ('195', 81.78),\n ('196', 76.58),\n ('197', 75.92),\n ('198', 46.35),\n ('199', 78.27),\n ('200', 92.57)]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = pd.read_excel('./data/data/NEW_IRB300012145_Patient_ID_deidentified.xlsx')\n",
    "smoking_status = labels_df.iloc[:, [0,27]]  # 22 33 27\n",
    "smoking_status.iloc[:, 0] = smoking_status.iloc[:, 0].apply(lambda x: x[-3:])\n",
    "\n",
    "filtered_df = smoking_status[~pd.isna(smoking_status.iloc[:, 1])]\n",
    "\n",
    "label_list = list(filtered_df.itertuples(index=False, name=None))\n",
    "label_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:24:59.554726700Z",
     "start_time": "2024-07-31T14:24:58.868666700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "class AmplitudeNormalization:\n",
    "    def __call__(self, waveform):\n",
    "        # Normalize the waveform to be within [-1, 1]\n",
    "        peak = waveform.abs().max()\n",
    "        if peak > 0:\n",
    "            waveform = waveform / peak\n",
    "        return waveform\n",
    "\n",
    "# To use it:\n",
    "# waveform, sample_rate = torchaudio.load('path/to/audio.wav')\n",
    "# waveform = AmplitudeNormalization()(waveform)\n",
    "\n",
    "\n",
    "class PadTrimAudio:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, waveform):\n",
    "        if waveform.size(1) > self.max_len:\n",
    "            # Trim the waveform if longer than max_len\n",
    "            waveform = waveform[:, :self.max_len]\n",
    "        elif waveform.size(1) < self.max_len:\n",
    "            # Pad with zeros if shorter than max_len\n",
    "            padding_size = self.max_len - waveform.size(1)\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding_size), \"constant\", 0)\n",
    "\n",
    "        if waveform.size(0) < 128:\n",
    "            padding_size = 128 - waveform.size(0)\n",
    "            waveform = torch.nn.functional.pad(waveform, (padding_size, 0), \"constant\", 0)\n",
    "        return waveform\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "class FeatureNormalization:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, features):\n",
    "        # Fit the scaler on the training set features\n",
    "        self.scaler.fit(features)\n",
    "\n",
    "    def transform(self, features):\n",
    "        # Apply normalization to features\n",
    "        return self.scaler.transform(features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:25:03.896744700Z",
     "start_time": "2024-07-31T14:25:01.143009300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "label_to_int = {\n",
    "    'Normal': 0,\n",
    "    'Restrictive vent. Defect': 1,\n",
    "    'Obstructive vent. Defect': 1,\n",
    "    'Obstr. Cannot R/O Restriction': 1\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class SoundDataset(Dataset):\n",
    "    def __init__(self, data_dir, labels_df, transform=None, max_len=1000):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels_df = labels_df\n",
    "        self.transform = transform\n",
    "        self.max_len = max_len\n",
    "        self.orderlist = ['RLP.wav', 'RUP.wav', 'RUA Hum.wav', 'LUA Hum.wav', 'RUA.wav', 'RMP.wav', 'LMP.wav', 'LUA.wav', 'LLP.wav', 'LUP.wav']\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.labels_df[idx][0]\n",
    "        audio_dir = os.path.join(self.data_dir, str(patient_id), 'breath Eko')\n",
    "        # audio_dir = os.path.join(self.data_dir, str(patient_id), 'Voice')\n",
    "        audio_files = [os.path.join(audio_dir, f) for f in os.listdir(audio_dir) if f.endswith('.wav')]\n",
    "\n",
    "        # Concatenate audio files\n",
    "        waveform_list = [torch.zeros(128, 1000) for _ in range(len(self.orderlist))]\n",
    "        for audio_file in audio_files:\n",
    "            file_name = os.path.basename(audio_file)\n",
    "            index = self.orderlist.index(file_name)\n",
    "\n",
    "\n",
    "            y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "            # STFT\n",
    "            D = librosa.stft(y, n_fft=2048, hop_length=512)  # n_fft and hop_length can be adjusted based on your needs\n",
    "            S = np.abs(D) ** 2  # Convert to power spectrum\n",
    "            mel_spec = librosa.feature.melspectrogram(S=S, sr=sr, n_mels=128)\n",
    "\n",
    "            # print(mel_spec)\n",
    "            # plt.figure(figsize=(10, 4))\n",
    "            # librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max), sr=sr, hop_length=512, x_axis='time', y_axis='mel')\n",
    "            # plt.colorbar(format='%+2.0f dB')\n",
    "            # plt.title('Mel spectrogram')\n",
    "            # plt.tight_layout()\n",
    "            # plt.show()\n",
    "            # print(len(y))\n",
    "            # print(sr)\n",
    "\n",
    "            # if self.transform:\n",
    "            #     waveform = self.transform(waveform)\n",
    "            waveform = PadTrimAudio(self.max_len)(torch.tensor(mel_spec))\n",
    "            waveform_list[index] = waveform\n",
    "\n",
    "\n",
    "        # Concatenate all waveforms along the time dimension\n",
    "        waveform = torch.cat(waveform_list, dim=0)\n",
    "\n",
    "        label = self.labels_df[idx][1]\n",
    "\n",
    "        return waveform, label\n",
    "\n",
    "transform = AmplitudeNormalization()\n",
    "data_dir = './data/data/Patients'\n",
    "sound_dataset = SoundDataset(data_dir, label_list, transform=transform)\n",
    "\n",
    "all_len =  len(label_list)\n",
    "train_len = round(all_len * 0.9)\n",
    "\n",
    "train_dataset, test_dataset = random_split(sound_dataset, [train_len, all_len-train_len])\n",
    "\n",
    "traindataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "testdataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "dataloader = DataLoader(sound_dataset, batch_size=20, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:32:20.334561400Z",
     "start_time": "2024-07-31T14:32:20.325243100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1280, 1000])\n",
      "tensor([69.5500, 79.5800, 81.2700, 90.6000], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([67.7200, 80.9800, 76.7000, 85.0100], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([80.2000, 76.9100, 68.5100, 81.4500], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([70.7700, 74.7900, 86.1500, 79.0100], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([88.7500, 57.3000, 83.7700, 91.3100], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([78.8600, 75.9600, 76.7200, 59.5900], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([54.1900, 81.9900, 72.9800, 71.7700], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([73.0800, 86.2900, 75.9700, 87.5300], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([62.8400, 84.4400, 69.3900, 72.2300], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([80.7300, 96.5100, 80.3600, 83.7200], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([75.5000, 84.7800, 79.3000, 80.2500], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([55.0000, 76.3400, 67.0700, 47.4700], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([82.6200, 66.1300, 88.5300, 88.7900], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([60.9500, 62.5400, 78.2100, 75.3900], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([93.3300, 81.7800, 76.5800, 75.6900], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([73.3900, 83.2500, 84.2500, 79.3300], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([78.1500, 61.4700, 89.5800, 79.1200], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([74.0500, 80.3000, 78.8100, 67.4700], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([44.0500, 77.9300, 92.5700, 83.3200], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([77.9800, 69.1100, 71.0400, 70.6200], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([94.6100, 76.8900, 75.5800, 76.5700], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([65.1600, 33.9900, 46.5100, 71.2700], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([74.1600, 84.6600, 84.7200, 82.1600], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([92.2200, 67.5900, 79.4400, 83.9600], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([74.3900, 58.6300, 91.9000, 43.9400], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([75.9200, 79.5100, 84.7000, 83.7100], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([81.5000, 83.8600, 77.1600, 71.8800], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([84.1200, 28.3000, 75.6800, 62.9000], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([80.0500, 79.8700, 73.8600, 81.0600], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([87.9500, 79.3000, 39.1000, 51.3000], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([46.5800, 79.5100, 89.2200, 38.7500], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([75.7200, 63.7800, 71.8600, 81.7900], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([82.5900, 37.1700, 76.8900, 30.8800], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([85.9100, 69.8700, 78.2700, 73.2900], dtype=torch.float64)\n",
      "torch.Size([1, 1280, 1000])\n",
      "tensor([63.7800], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for batch in traindataloader:\n",
    "    waveforms, labels = batch\n",
    "    print(waveforms.shape)\n",
    "    print(labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1280, 1000])\n",
      "tensor([59.0000, 81.9900, 79.1600, 74.8800], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([80.0900, 87.5900, 83.6500, 54.0800], dtype=torch.float64)\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([46.3500, 79.0500, 81.2300, 84.1000], dtype=torch.float64)\n",
      "torch.Size([3, 1280, 1000])\n",
      "tensor([69.4000, 68.8900, 81.5000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for batch in testdataloader:\n",
    "    waveforms, labels = batch\n",
    "    print(waveforms.shape)\n",
    "    print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:32:46.486415500Z",
     "start_time": "2024-07-31T14:32:44.989845200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        # Adjusted the number of input channels to 1280\n",
    "        self.conv1 = nn.Conv1d(1280, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2, 2)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Calculate the size of the output from the last conv layer to pass to the first linear layer\n",
    "        initial_length = 1000  # Initial length of the signal\n",
    "        final_conv_length = self._get_conv1d_output_size(initial_length, 3, 1, 1, 2)  # After first conv and pool\n",
    "        final_conv_length = self._get_conv1d_output_size(final_conv_length, 3, 1, 1, 2)  # After second conv and pool\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * final_conv_length, 128)  # Linear layer for feature reduction\n",
    "        self.fc2 = nn.Linear(128, 1)  # Output layer for 3 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # Flatten from the second dimension onward\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def _get_conv1d_output_size(self, L, kernel_size, stride, padding, pool):\n",
    "        # Utility function to calculate the size of the output of a conv1d layer after pooling\n",
    "        L = (L + 2 * padding - kernel_size) // stride + 1\n",
    "        L = L // pool\n",
    "        return L\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        # Flatten the input from 1280x1000 to 1280000\n",
    "        self.fc1 = nn.Linear(1280 * 1000, 1024)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(1024, 512)          # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(512, 3)             # Output layer for 3 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input\n",
    "        x = x.view(-1, 1280 * 1000)  # Ensure input tensor is reshaped to (batch_size, 1280*1000)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation function here as it will be used with CrossEntropyLoss\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:35:42.434587900Z",
     "start_time": "2024-07-31T14:35:42.429998100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def test_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Tests the given model on the provided test data loader.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The model to test.\n",
    "        test_loader (torch.utils.data.DataLoader): DataLoader for the test set.\n",
    "        device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_mse = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            print(labels)\n",
    "            print(outputs.squeeze())\n",
    "            criterion = nn.MSELoss()\n",
    "            all_mse += criterion(outputs.squeeze(), labels)\n",
    "            print('----------------------------------------------')\n",
    "        print(all_mse)\n",
    "\n",
    "\n",
    "    accuracy = 0\n",
    "\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:35:43.742251500Z",
     "start_time": "2024-07-31T14:35:43.734089800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3541.0961\n",
      "tensor([59.0000, 81.9900, 79.1600, 74.8800], dtype=torch.float64)\n",
      "tensor([58.8816, 37.0917, 36.1767, 33.0243])\n",
      "----------------------------------------------\n",
      "tensor([80.0900, 87.5900, 83.6500, 54.0800], dtype=torch.float64)\n",
      "tensor([47.8499, 37.5833, 34.9135, 56.4447])\n",
      "----------------------------------------------\n",
      "tensor([46.3500, 79.0500, 81.2300, 84.1000], dtype=torch.float64)\n",
      "tensor([36.3187, 35.6570, 76.3044, 33.2517])\n",
      "----------------------------------------------\n",
      "tensor([69.4000, 68.8900, 81.5000], dtype=torch.float64)\n",
      "tensor([38.7508, 44.8691, 49.3600])\n",
      "----------------------------------------------\n",
      "tensor(4882.1978, dtype=torch.float64)\n",
      "Epoch 2, Loss: 627.8464\n",
      "tensor([59.0000, 81.9900, 79.1600, 74.8800], dtype=torch.float64)\n",
      "tensor([82.9321, 68.4615, 69.4250, 66.6730])\n",
      "----------------------------------------------\n",
      "tensor([80.0900, 87.5900, 83.6500, 54.0800], dtype=torch.float64)\n",
      "tensor([80.0320, 69.6520, 68.1879, 85.2717])\n",
      "----------------------------------------------\n",
      "tensor([46.3500, 79.0500, 81.2300, 84.1000], dtype=torch.float64)\n",
      "tensor([69.2688, 65.9385, 87.2323, 66.8889])\n",
      "----------------------------------------------\n",
      "tensor([69.4000, 68.8900, 81.5000], dtype=torch.float64)\n",
      "tensor([71.6241, 69.8750, 74.1422])\n",
      "----------------------------------------------\n",
      "tensor(890.2923, dtype=torch.float64)\n",
      "Epoch 3, Loss: 197.6886\n",
      "tensor([59.0000, 81.9900, 79.1600, 74.8800], dtype=torch.float64)\n",
      "tensor([75.0530, 69.0117, 70.3256, 69.3122])\n",
      "----------------------------------------------\n",
      "tensor([80.0900, 87.5900, 83.6500, 54.0800], dtype=torch.float64)\n",
      "tensor([74.2229, 70.3161, 69.7679, 76.1622])\n",
      "----------------------------------------------\n",
      "tensor([46.3500, 79.0500, 81.2300, 84.1000], dtype=torch.float64)\n",
      "tensor([70.2918, 69.2354, 80.6193, 69.3033])\n",
      "----------------------------------------------\n",
      "tensor([69.4000, 68.8900, 81.5000], dtype=torch.float64)\n",
      "tensor([71.2812, 70.9654, 73.8847])\n",
      "----------------------------------------------\n",
      "tensor(631.2415, dtype=torch.float64)\n",
      "Epoch 4, Loss: 186.6891\n",
      "tensor([59.0000, 81.9900, 79.1600, 74.8800], dtype=torch.float64)\n",
      "tensor([72.9427, 69.3360, 70.8459, 70.1236])\n",
      "----------------------------------------------\n",
      "tensor([80.0900, 87.5900, 83.6500, 54.0800], dtype=torch.float64)\n",
      "tensor([73.0744, 70.5136, 70.3359, 73.5054])\n",
      "----------------------------------------------\n",
      "tensor([46.3500, 79.0500, 81.2300, 84.1000], dtype=torch.float64)\n",
      "tensor([70.6335, 68.9459, 73.7063, 70.0644])\n",
      "----------------------------------------------\n",
      "tensor([69.4000, 68.8900, 81.5000], dtype=torch.float64)\n",
      "tensor([71.3076, 71.1898, 73.0430])\n",
      "----------------------------------------------\n",
      "tensor(598.5892, dtype=torch.float64)\n",
      "Epoch 5, Loss: 201.9670\n",
      "tensor([59.0000, 81.9900, 79.1600, 74.8800], dtype=torch.float64)\n",
      "tensor([74.4253, 71.8575, 73.0410, 72.2918])\n",
      "----------------------------------------------\n",
      "tensor([80.0900, 87.5900, 83.6500, 54.0800], dtype=torch.float64)\n",
      "tensor([74.9562, 72.5144, 72.4892, 74.8445])\n",
      "----------------------------------------------\n",
      "tensor([46.3500, 79.0500, 81.2300, 84.1000], dtype=torch.float64)\n",
      "tensor([72.7672, 71.9855, 81.7943, 72.2469])\n",
      "----------------------------------------------\n",
      "tensor([69.4000, 68.8900, 81.5000], dtype=torch.float64)\n",
      "tensor([73.3672, 73.7912, 76.2760])\n",
      "----------------------------------------------\n",
      "tensor(543.0240, dtype=torch.float64)\n",
      "Epoch 6, Loss: 171.3619\n",
      "tensor([59.0000, 81.9900, 79.1600, 74.8800], dtype=torch.float64)\n",
      "tensor([73.7439, 71.7692, 73.0736, 72.4552])\n",
      "----------------------------------------------\n",
      "tensor([80.0900, 87.5900, 83.6500, 54.0800], dtype=torch.float64)\n",
      "tensor([74.7287, 71.8671, 72.5000, 72.6270])\n",
      "----------------------------------------------\n",
      "tensor([46.3500, 79.0500, 81.2300, 84.1000], dtype=torch.float64)\n",
      "tensor([72.9499, 71.9462, 78.4350, 72.3687])\n",
      "----------------------------------------------\n",
      "tensor([69.4000, 68.8900, 81.5000], dtype=torch.float64)\n",
      "tensor([73.1140, 73.9301, 75.2007])\n",
      "----------------------------------------------\n",
      "tensor(529.4165, dtype=torch.float64)\n",
      "Epoch 7, Loss: 152.4200\n",
      "tensor([59.0000, 81.9900, 79.1600, 74.8800], dtype=torch.float64)\n",
      "tensor([74.9243, 73.8882, 75.1835, 74.5414])\n",
      "----------------------------------------------\n",
      "tensor([80.0900, 87.5900, 83.6500, 54.0800], dtype=torch.float64)\n",
      "tensor([76.6491, 73.4617, 74.4484, 72.5387])\n",
      "----------------------------------------------\n",
      "tensor([46.3500, 79.0500, 81.2300, 84.1000], dtype=torch.float64)\n",
      "tensor([74.8934, 74.9563, 79.4434, 74.4058])\n",
      "----------------------------------------------\n",
      "tensor([69.4000, 68.8900, 81.5000], dtype=torch.float64)\n",
      "tensor([74.9390, 77.5832, 77.6383])\n",
      "----------------------------------------------\n",
      "tensor(515.5499, dtype=torch.float64)\n",
      "Epoch 8, Loss: 135.4029\n",
      "tensor([59.0000, 81.9900, 79.1600, 74.8800], dtype=torch.float64)\n",
      "tensor([78.2254, 76.8550, 77.7925, 77.0055])\n",
      "----------------------------------------------\n",
      "tensor([80.0900, 87.5900, 83.6500, 54.0800], dtype=torch.float64)\n",
      "tensor([79.5695, 75.2262, 76.6957, 73.2383])\n",
      "----------------------------------------------\n",
      "tensor([46.3500, 79.0500, 81.2300, 84.1000], dtype=torch.float64)\n",
      "tensor([77.4952, 77.0508, 88.3917, 76.8321])\n",
      "----------------------------------------------\n",
      "tensor([69.4000, 68.8900, 81.5000], dtype=torch.float64)\n",
      "tensor([76.8321, 79.7150, 82.1155])\n",
      "----------------------------------------------\n",
      "tensor(569.8592, dtype=torch.float64)\n",
      "Epoch 9, Loss: 147.4578\n",
      "tensor([59.0000, 81.9900, 79.1600, 74.8800], dtype=torch.float64)\n",
      "tensor([73.1434, 75.0796, 77.2849, 76.6212])\n",
      "----------------------------------------------\n",
      "tensor([80.0900, 87.5900, 83.6500, 54.0800], dtype=torch.float64)\n",
      "tensor([79.0342, 74.3697, 76.0415, 67.9594])\n",
      "----------------------------------------------\n",
      "tensor([46.3500, 79.0500, 81.2300, 84.1000], dtype=torch.float64)\n",
      "tensor([76.8088, 76.3326, 72.2173, 76.3895])\n",
      "----------------------------------------------\n",
      "tensor([69.4000, 68.8900, 81.5000], dtype=torch.float64)\n",
      "tensor([75.0382, 80.9284, 78.6663])\n",
      "----------------------------------------------\n",
      "tensor(500.7205, dtype=torch.float64)\n",
      "Epoch 10, Loss: 127.8193\n",
      "tensor([59.0000, 81.9900, 79.1600, 74.8800], dtype=torch.float64)\n",
      "tensor([70.7033, 74.6285, 76.5059, 75.3737])\n",
      "----------------------------------------------\n",
      "tensor([80.0900, 87.5900, 83.6500, 54.0800], dtype=torch.float64)\n",
      "tensor([78.7869, 72.6836, 74.7275, 63.6857])\n",
      "----------------------------------------------\n",
      "tensor([46.3500, 79.0500, 81.2300, 84.1000], dtype=torch.float64)\n",
      "tensor([75.5774, 75.2384, 78.7917, 75.1150])\n",
      "----------------------------------------------\n",
      "tensor([69.4000, 68.8900, 81.5000], dtype=torch.float64)\n",
      "tensor([74.0178, 78.7210, 78.9462])\n",
      "----------------------------------------------\n",
      "tensor(428.9159, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AudioClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            data = data.to(device).float()  # Convert data to float\n",
    "            targets = targets.to(device).float()  # Convert targets to float\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)  # Add channel dimension\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')\n",
    "        accuracy = test_model(model, testdataloader, device)\n",
    "\n",
    "train(model, device, traindataloader, optimizer, criterion)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:38:15.048564300Z",
     "start_time": "2024-07-31T14:35:46.996449300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
