{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "'never'"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = pd.read_excel('./data/NEW_IRB300012145_Patient_ID_deidentified.xlsx')\n",
    "Smokeing_status = labels_df.iloc[:, 4].to_list()\n",
    "\n",
    "Smokeing_status[149]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "class AmplitudeNormalization:\n",
    "    def __call__(self, waveform):\n",
    "        # Normalize the waveform to be within [-1, 1]\n",
    "        peak = waveform.abs().max()\n",
    "        if peak > 0:\n",
    "            waveform = waveform / peak\n",
    "        return waveform\n",
    "\n",
    "# To use it:\n",
    "# waveform, sample_rate = torchaudio.load('path/to/audio.wav')\n",
    "# waveform = AmplitudeNormalization()(waveform)\n",
    "\n",
    "\n",
    "class PadTrimAudio:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, waveform):\n",
    "        if waveform.size(1) > self.max_len:\n",
    "            # Trim the waveform if longer than max_len\n",
    "            waveform = waveform[:, :self.max_len]\n",
    "        elif waveform.size(1) < self.max_len:\n",
    "            # Pad with zeros if shorter than max_len\n",
    "            padding_size = self.max_len - waveform.size(1)\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding_size), \"constant\", 0)\n",
    "        return waveform\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "class FeatureNormalization:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, features):\n",
    "        # Fit the scaler on the training set features\n",
    "        self.scaler.fit(features)\n",
    "\n",
    "    def transform(self, features):\n",
    "        # Apply normalization to features\n",
    "        return self.scaler.transform(features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "class SoundDataset(Dataset):\n",
    "    def __init__(self, data_dir, labels_df, transform=None, max_len=1000000):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels_df = labels_df\n",
    "        self.transform = transform\n",
    "        self.max_len = max_len\n",
    "        self.orderlist = ['RLP.wav', 'RUP.wav', 'RUA Hum.wav', 'LUA Hum.wav', 'RUA.wav', 'RMP.wav', 'LMP.wav', 'LUA.wav', 'LLP.wav', 'LUP.wav']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = str(idx+1).zfill(3)\n",
    "        audio_dir = os.path.join(self.data_dir, str(patient_id), 'breath Eko')\n",
    "        audio_files = [os.path.join(audio_dir, f) for f in os.listdir(audio_dir) if f.endswith('.wav')]\n",
    "\n",
    "        # Concatenate audio files\n",
    "        waveform_list = [torch.zeros(1, self.max_len) for _ in range(len(self.orderlist))]\n",
    "        for audio_file in audio_files:\n",
    "            file_name = os.path.basename(audio_file)\n",
    "            index = self.orderlist.index(file_name)\n",
    "            waveform, sample_rate = torchaudio.load(audio_file)\n",
    "            if self.transform:\n",
    "                waveform = self.transform(waveform)\n",
    "            waveform = PadTrimAudio(self.max_len)(waveform)\n",
    "            waveform_list[index] = waveform\n",
    "\n",
    "\n",
    "        # Concatenate all waveforms along the time dimension\n",
    "        waveform = torch.cat(waveform_list, dim=0)\n",
    "\n",
    "        label = self.labels_df[idx]\n",
    "\n",
    "        return waveform, label\n",
    "\n",
    "transform = AmplitudeNormalization()\n",
    "data_dir = './data/Patients'\n",
    "sound_dataset = SoundDataset(data_dir, Smokeing_status, transform=transform)\n",
    "\n",
    "dataloader = DataLoader(sound_dataset, batch_size=4, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 1000000])\n",
      "('never', 'never', 'never', 'former')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('never', 'never', 'never', 'former')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('former', 'current', 'never', 'former')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('former', 'never', 'never', 'never')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('former', 'never', 'former', 'former')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('never', 'never', 'never', 'former')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('never', 'never', 'never', 'former')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('former', 'never', 'former', 'never')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('never', 'never', 'never', 'former')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('never', 'never', 'never', 'current')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('never', 'never', 'never', 'never')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('never', 'never', 'former', 'never')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('former', 'never', 'former', 'never')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('former', 'never', 'former', 'never')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('never', 'current', 'former', 'never')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('former', 'never', 'former', 'never')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('former', 'never', 'never', 'former')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('never', 'never', 'never', 'never')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('never', 'former', 'never', 'current')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('former', 'never', 'never', 'never')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('never', 'never', 'former', 'never')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('never', 'never', 'never', 'never')\n",
      "torch.Size([4, 10, 1000000])\n",
      "('never', 'never', 'never', 'never')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [10, 1000000] at entry 0 and [13, 1000000] at entry 2",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[1;32m      2\u001B[0m     waveforms, labels \u001B[38;5;241m=\u001B[39m batch\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(waveforms\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:265\u001B[0m, in \u001B[0;36mdefault_collate\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[1;32m    205\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001B[39;00m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001B[0m, in \u001B[0;36mcollate\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    139\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m--> 142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    139\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m--> 142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:119\u001B[0m, in \u001B[0;36mcollate\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m--> 119\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m    122\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:162\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    160\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    161\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[0;32m--> 162\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: stack expects each tensor to be equal size, but got [10, 1000000] at entry 0 and [13, 1000000] at entry 2"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    waveforms, labels = batch\n",
    "    print(waveforms.shape)\n",
    "    print(labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "class FixedSizeMelSpectrogram(T.MelSpectrogram):\n",
    "    def __init__(self, sample_rate=22050, n_mels=64, max_pad_length=900, n_fft=2048, hop_length=512, **kwargs):\n",
    "        super().__init__(sample_rate=sample_rate, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, **kwargs)\n",
    "        self.max_pad_length = max_pad_length\n",
    "\n",
    "    def forward(self, waveform):\n",
    "        # Convert stereo to mono if necessary\n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        # Generate the Mel Spectrogram\n",
    "        mel_spec = super().forward(waveform)\n",
    "\n",
    "        # Ensure the Mel Spectrogram is of fixed size\n",
    "        if mel_spec.size(2) < self.max_pad_length:\n",
    "            # Padding\n",
    "            padded_mel_spec = torch.nn.functional.pad(mel_spec, (0, self.max_pad_length - mel_spec.size(2)))\n",
    "        else:\n",
    "            # Truncating\n",
    "            padded_mel_spec = mel_spec[:, :, :self.max_pad_length]\n",
    "\n",
    "        return padded_mel_spec\n",
    "\n",
    "label_to_int = {\n",
    "    'current': 0,\n",
    "    'former': 1,\n",
    "    'never': 2\n",
    "}\n",
    "\n",
    "\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(115200, 128)  # Adjust size based on your MelSpectrogram output\n",
    "        self.fc2 = nn.Linear(128, 10)  # Assuming 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class UrbanSound8KDataset(Dataset):\n",
    "    def __init__(self, soundata_dataset, transform=None, target_length=1000000):  # target_length depends on your data\n",
    "        self.clips = soundata_dataset.load_clips()\n",
    "        self.transform = transform\n",
    "        self.target_length = target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clips)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clip_key = list(self.clips.keys())[idx]\n",
    "        clip = self.clips[clip_key]\n",
    "        waveform, sr = torchaudio.load(clip.audio_path)\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "\n",
    "        label_string = clip.tags.labels[0]\n",
    "        label = label_to_int[label_string]\n",
    "\n",
    "        return waveform, label\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "\n",
    "# Usage of the transform\n",
    "transform = FixedSizeMelSpectrogram(sample_rate=22050, n_mels=64, max_pad_length=900)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = UrbanSound8KDataset(dataset, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 12.5799, Accuracy: 33.50%\n",
      "Epoch 2, Loss: 1.5315, Accuracy: 51.82%\n",
      "Epoch 3, Loss: 1.4019, Accuracy: 61.37%\n",
      "Epoch 4, Loss: 1.1271, Accuracy: 65.60%\n",
      "Epoch 5, Loss: 1.2163, Accuracy: 69.84%\n",
      "Epoch 6, Loss: 1.0536, Accuracy: 70.11%\n",
      "Epoch 7, Loss: 0.9006, Accuracy: 77.40%\n",
      "Epoch 8, Loss: 0.7792, Accuracy: 80.08%\n",
      "Epoch 9, Loss: 0.8212, Accuracy: 82.48%\n",
      "Epoch 10, Loss: 0.7381, Accuracy: 83.67%\n",
      "Epoch 11, Loss: 0.4484, Accuracy: 88.31%\n",
      "Epoch 12, Loss: 0.3376, Accuracy: 91.30%\n",
      "Epoch 13, Loss: 0.7144, Accuracy: 88.99%\n",
      "Epoch 14, Loss: 0.7038, Accuracy: 87.44%\n",
      "Epoch 15, Loss: 0.7641, Accuracy: 86.21%\n",
      "Epoch 16, Loss: 0.3453, Accuracy: 93.19%\n",
      "Epoch 17, Loss: 0.2770, Accuracy: 93.84%\n",
      "Epoch 18, Loss: 0.1458, Accuracy: 95.98%\n",
      "Epoch 19, Loss: 0.8000, Accuracy: 92.24%\n",
      "Epoch 20, Loss: 0.3702, Accuracy: 94.32%\n",
      "Epoch 21, Loss: 0.3860, Accuracy: 95.68%\n",
      "Epoch 22, Loss: 0.3826, Accuracy: 94.31%\n",
      "Epoch 23, Loss: 0.0996, Accuracy: 97.62%\n",
      "Epoch 24, Loss: 0.4093, Accuracy: 96.06%\n",
      "Epoch 25, Loss: 0.2236, Accuracy: 96.60%\n",
      "Epoch 26, Loss: 0.3254, Accuracy: 96.42%\n",
      "Epoch 27, Loss: 0.2713, Accuracy: 96.79%\n",
      "Epoch 28, Loss: 0.2692, Accuracy: 96.87%\n",
      "Epoch 29, Loss: 0.6702, Accuracy: 94.54%\n",
      "Epoch 30, Loss: 0.3222, Accuracy: 96.01%\n",
      "Epoch 31, Loss: 0.1846, Accuracy: 97.49%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AudioClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion, num_epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)  # Add channel dimension\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "train(model, device, train_loader, optimizer, criterion)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
