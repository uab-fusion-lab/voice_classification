{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO: NumExpr defaulting to 8 threads.\n",
      "INFO: Downloading ['all', 'index']. Index is being stored in D:\\anaconda\\Lib\\site-packages\\soundata\\datasets\\indexes, and the rest of files in /tmp\\sound_datasets\\urbansound8k\n",
      "INFO: [all] downloading UrbanSound8K.tar.gz\n",
      "5.61GB [10:54, 9.20MB/s]                                \n",
      "INFO: [index] downloading urbansound8k_index_1.0.json\n",
      "1.15MB [00:01, 817kB/s]                             \n",
      "100%|██████████| 1/1 [00:00<00:00, 62.05it/s]\n",
      "100%|██████████| 8732/8732 [00:45<00:00, 190.83it/s]\n",
      "INFO: Success: the dataset is complete and all files are valid.\n",
      "INFO: --------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "({'metadata': {}, 'clips': {}}, {'metadata': {}, 'clips': {}})"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import soundata\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = soundata.initialize('urbansound8k')\n",
    "\n",
    "# Download the dataset\n",
    "dataset.download()  # You can specify the download location with the `data_home` parameter.\n",
    "\n",
    "# Validate that all the expected files are there\n",
    "dataset.validate()  # This will check if all files expected by the dataset are present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Downloading ['all', 'index']. Index is being stored in D:\\anaconda\\Lib\\site-packages\\soundata\\datasets\\indexes, and the rest of files in /tmp\\sound_datasets\\urbansound8k\n",
      "INFO: [all] downloading UrbanSound8K.tar.gz\n",
      "INFO: /tmp\\sound_datasets\\urbansound8k\\UrbanSound8K.tar.gz already exists and will not be downloaded. Rerun with force_overwrite=True to delete this file and force the download.\n",
      "INFO: /tmp\\sound_datasets\\urbansound8k\\audio already exists. Run with force_overwrite=True to download from scratch\n",
      "INFO: /tmp\\sound_datasets\\urbansound8k\\FREESOUNDCREDITS.txt already exists. Run with force_overwrite=True to download from scratch\n",
      "INFO: /tmp\\sound_datasets\\urbansound8k\\metadata already exists. Run with force_overwrite=True to download from scratch\n",
      "INFO: /tmp\\sound_datasets\\urbansound8k\\UrbanSound8K_README.txt already exists. Run with force_overwrite=True to download from scratch\n",
      "INFO: [index] downloading urbansound8k_index_1.0.json\n",
      "INFO: D:\\anaconda\\Lib\\site-packages\\soundata\\datasets\\indexes\\urbansound8k_index_1.0.json already exists and will not be downloaded. Rerun with force_overwrite=True to delete this file and force the download.\n",
      "100%|██████████| 1/1 [00:00<00:00, 63.14it/s]\n",
      "100%|██████████| 8732/8732 [00:15<00:00, 548.99it/s]\n",
      "INFO: Success: the dataset is complete and all files are valid.\n",
      "INFO: --------------------\n"
     ]
    }
   ],
   "source": [
    "import soundata\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = soundata.initialize('urbansound8k')\n",
    "dataset.download()\n",
    "dataset.validate()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "class FixedSizeMelSpectrogram(T.MelSpectrogram):\n",
    "    def __init__(self, sample_rate=22050, n_mels=64, max_pad_length=900, n_fft=2048, hop_length=512, **kwargs):\n",
    "        super().__init__(sample_rate=sample_rate, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, **kwargs)\n",
    "        self.max_pad_length = max_pad_length\n",
    "\n",
    "    def forward(self, waveform):\n",
    "        # Convert stereo to mono if necessary\n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        # Generate the Mel Spectrogram\n",
    "        mel_spec = super().forward(waveform)\n",
    "\n",
    "        # Ensure the Mel Spectrogram is of fixed size\n",
    "        if mel_spec.size(2) < self.max_pad_length:\n",
    "            # Padding\n",
    "            padded_mel_spec = torch.nn.functional.pad(mel_spec, (0, self.max_pad_length - mel_spec.size(2)))\n",
    "        else:\n",
    "            # Truncating\n",
    "            padded_mel_spec = mel_spec[:, :, :self.max_pad_length]\n",
    "\n",
    "        return padded_mel_spec\n",
    "\n",
    "label_to_int = {\n",
    "    'air_conditioner': 0,\n",
    "    'car_horn': 1,\n",
    "    'children_playing': 2,\n",
    "    'dog_bark': 3,\n",
    "    'drilling': 4,\n",
    "    'engine_idling': 5,\n",
    "    'gun_shot': 6,\n",
    "    'jackhammer': 7,\n",
    "    'siren': 8,\n",
    "    'street_music': 9\n",
    "}\n",
    "\n",
    "\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(115200, 128)  # Adjust size based on your MelSpectrogram output\n",
    "        self.fc2 = nn.Linear(128, 10)  # Assuming 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class UrbanSound8KDataset(Dataset):\n",
    "    def __init__(self, soundata_dataset, transform=None, target_length=1000000):  # target_length depends on your data\n",
    "        self.clips = soundata_dataset.load_clips()\n",
    "        self.transform = transform\n",
    "        self.target_length = target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clips)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clip_key = list(self.clips.keys())[idx]\n",
    "        clip = self.clips[clip_key]\n",
    "        waveform, sr = torchaudio.load(clip.audio_path)\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "\n",
    "        label_string = clip.tags.labels[0]\n",
    "        label = label_to_int[label_string]\n",
    "\n",
    "        return waveform, label\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "\n",
    "# Usage of the transform\n",
    "transform = FixedSizeMelSpectrogram(sample_rate=22050, n_mels=64, max_pad_length=900)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = UrbanSound8KDataset(dataset, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 12.5799, Accuracy: 33.50%\n",
      "Epoch 2, Loss: 1.5315, Accuracy: 51.82%\n",
      "Epoch 3, Loss: 1.4019, Accuracy: 61.37%\n",
      "Epoch 4, Loss: 1.1271, Accuracy: 65.60%\n",
      "Epoch 5, Loss: 1.2163, Accuracy: 69.84%\n",
      "Epoch 6, Loss: 1.0536, Accuracy: 70.11%\n",
      "Epoch 7, Loss: 0.9006, Accuracy: 77.40%\n",
      "Epoch 8, Loss: 0.7792, Accuracy: 80.08%\n",
      "Epoch 9, Loss: 0.8212, Accuracy: 82.48%\n",
      "Epoch 10, Loss: 0.7381, Accuracy: 83.67%\n",
      "Epoch 11, Loss: 0.4484, Accuracy: 88.31%\n",
      "Epoch 12, Loss: 0.3376, Accuracy: 91.30%\n",
      "Epoch 13, Loss: 0.7144, Accuracy: 88.99%\n",
      "Epoch 14, Loss: 0.7038, Accuracy: 87.44%\n",
      "Epoch 15, Loss: 0.7641, Accuracy: 86.21%\n",
      "Epoch 16, Loss: 0.3453, Accuracy: 93.19%\n",
      "Epoch 17, Loss: 0.2770, Accuracy: 93.84%\n",
      "Epoch 18, Loss: 0.1458, Accuracy: 95.98%\n",
      "Epoch 19, Loss: 0.8000, Accuracy: 92.24%\n",
      "Epoch 20, Loss: 0.3702, Accuracy: 94.32%\n",
      "Epoch 21, Loss: 0.3860, Accuracy: 95.68%\n",
      "Epoch 22, Loss: 0.3826, Accuracy: 94.31%\n",
      "Epoch 23, Loss: 0.0996, Accuracy: 97.62%\n",
      "Epoch 24, Loss: 0.4093, Accuracy: 96.06%\n",
      "Epoch 25, Loss: 0.2236, Accuracy: 96.60%\n",
      "Epoch 26, Loss: 0.3254, Accuracy: 96.42%\n",
      "Epoch 27, Loss: 0.2713, Accuracy: 96.79%\n",
      "Epoch 28, Loss: 0.2692, Accuracy: 96.87%\n",
      "Epoch 29, Loss: 0.6702, Accuracy: 94.54%\n",
      "Epoch 30, Loss: 0.3222, Accuracy: 96.01%\n",
      "Epoch 31, Loss: 0.1846, Accuracy: 97.49%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AudioClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion, num_epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)  # Add channel dimension\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "train(model, device, train_loader, optimizer, criterion)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
