{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'never'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = pd.read_excel('./data/NEW_IRB300012145_Patient_ID_deidentified.xlsx')\n",
    "Smokeing_status = labels_df.iloc[:, 4].to_list()\n",
    "\n",
    "Smokeing_status[149]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "class AmplitudeNormalization:\n",
    "    def __call__(self, waveform):\n",
    "        # Normalize the waveform to be within [-1, 1]\n",
    "        peak = waveform.abs().max()\n",
    "        if peak > 0:\n",
    "            waveform = waveform / peak\n",
    "        return waveform\n",
    "\n",
    "# To use it:\n",
    "# waveform, sample_rate = torchaudio.load('path/to/audio.wav')\n",
    "# waveform = AmplitudeNormalization()(waveform)\n",
    "\n",
    "\n",
    "class PadTrimAudio:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, waveform):\n",
    "        if waveform.size(1) > self.max_len:\n",
    "            # Trim the waveform if longer than max_len\n",
    "            waveform = waveform[:, :self.max_len]\n",
    "        elif waveform.size(1) < self.max_len:\n",
    "            # Pad with zeros if shorter than max_len\n",
    "            padding_size = self.max_len - waveform.size(1)\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding_size), \"constant\", 0)\n",
    "\n",
    "        if waveform.size(0) < 128:\n",
    "            padding_size = 128 - waveform.size(0)\n",
    "            waveform = torch.nn.functional.pad(waveform, (padding_size, 0), \"constant\", 0)\n",
    "        return waveform\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "class FeatureNormalization:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, features):\n",
    "        # Fit the scaler on the training set features\n",
    "        self.scaler.fit(features)\n",
    "\n",
    "    def transform(self, features):\n",
    "        # Apply normalization to features\n",
    "        return self.scaler.transform(features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_int = {\n",
    "    'current': 0,\n",
    "    'former': 1,\n",
    "    'never': 2\n",
    "}\n",
    "\n",
    "label_to_int['current']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class SoundDataset(Dataset):\n",
    "    def __init__(self, data_dir, labels_df, transform=None, max_len=1000):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels_df = labels_df\n",
    "        self.transform = transform\n",
    "        self.max_len = max_len\n",
    "        self.orderlist = ['RLP.wav', 'RUP.wav', 'RUA Hum.wav', 'LUA Hum.wav', 'RUA.wav', 'RMP.wav', 'LMP.wav', 'LUA.wav', 'LLP.wav', 'LUP.wav']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = str(idx+1).zfill(3)\n",
    "        audio_dir = os.path.join(self.data_dir, str(patient_id), 'breath Eko')\n",
    "        audio_files = [os.path.join(audio_dir, f) for f in os.listdir(audio_dir) if f.endswith('.wav')]\n",
    "\n",
    "        # Concatenate audio files\n",
    "        waveform_list = [torch.zeros(128, 1000) for _ in range(len(self.orderlist))]\n",
    "        for audio_file in audio_files:\n",
    "            file_name = os.path.basename(audio_file)\n",
    "            index = self.orderlist.index(file_name)\n",
    "            # waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "\n",
    "\n",
    "            y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "            # STFT\n",
    "            D = librosa.stft(y, n_fft=2048, hop_length=512)  # n_fft and hop_length can be adjusted based on your needs\n",
    "            S = np.abs(D) ** 2  # Convert to power spectrum\n",
    "            mel_spec = librosa.feature.melspectrogram(S=S, sr=sr, n_mels=128)\n",
    "            if mel_spec.shape[0] != 128:\n",
    "                s = 10000\n",
    "            # print(mel_spec)\n",
    "            # plt.figure(figsize=(10, 4))\n",
    "            # librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max), sr=sr, hop_length=512, x_axis='time', y_axis='mel')\n",
    "            # plt.colorbar(format='%+2.0f dB')\n",
    "            # plt.title('Mel spectrogram')\n",
    "            # plt.tight_layout()\n",
    "            # plt.show()\n",
    "            # print(len(y))\n",
    "            # print(sr)\n",
    "\n",
    "            # if self.transform:\n",
    "            #     waveform = self.transform(waveform)\n",
    "            waveform = PadTrimAudio(self.max_len)(torch.tensor(mel_spec))\n",
    "            waveform_list[index] = waveform\n",
    "\n",
    "\n",
    "        # Concatenate all waveforms along the time dimension\n",
    "        waveform = torch.cat(waveform_list, dim=0)\n",
    "\n",
    "        label = label_to_int[self.labels_df[idx]]\n",
    "\n",
    "        return waveform, label\n",
    "\n",
    "transform = AmplitudeNormalization()\n",
    "data_dir = './data/Patients'\n",
    "sound_dataset = SoundDataset(data_dir, Smokeing_status, transform=transform)\n",
    "\n",
    "train_dataset, test_dataset = random_split(sound_dataset, [180, 20])\n",
    "\n",
    "traindataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "testdataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "dataloader = DataLoader(sound_dataset, batch_size=4, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 1, 2, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 2, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 0, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([1, 2, 1, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 1, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 1, 1, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([0, 1, 1, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 1, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 0, 1, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 1, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([1, 1, 2, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 1, 2, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([1, 2, 2, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 1, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 1, 1, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 1, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 2, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 1, 2, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 1, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 2, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 2, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengchenfan/miniconda3/lib/python3.10/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=190\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 2, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 2, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 0, 0])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 0, 2, 0])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([1, 2, 1, 0])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([0, 2, 0, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 2, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 1, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([1, 2, 2, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 2, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 2, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 2, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([1, 2, 1, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 1, 2, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 0, 1, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 1, 2, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([1, 2, 2, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([1, 2, 2, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 1, 2, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 2, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([1, 1, 1, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 2, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([1, 2, 1, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in traindataloader:\n",
    "    waveforms, labels = batch\n",
    "    print(waveforms.shape)\n",
    "    print(labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1280, 1000])\n",
      "tensor([1, 2, 1, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([1, 1, 1, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([2, 2, 2, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([0, 2, 1, 2])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([1, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in testdataloader:\n",
    "    waveforms, labels = batch\n",
    "    print(waveforms.shape)\n",
    "    print(labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(10, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2, 2)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Calculate the size of the output from the last conv layer to pass to the first linear layer\n",
    "        self.final_conv_length = self._get_conv1d_output_size(500000, 3, 1, 1, 2)  # output size after first conv and pool\n",
    "        self.final_conv_length = self._get_conv1d_output_size(self.final_conv_length, 3, 1, 1, 2)  # output size after second conv and pool\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * self.final_conv_length, 128)  # Linear layer for feature reduction\n",
    "        self.fc2 = nn.Linear(128, 3)  # Adjusting this for 3 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def _get_conv1d_output_size(self, L, kernel_size, stride, padding, pool):\n",
    "        # Utility function to calculate the size of the output of a conv1d layer after pooling\n",
    "        L = (L + 2 * padding - kernel_size) // stride + 1\n",
    "        L = L // pool\n",
    "        return L\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        # Flatten the input from 1280x1000 to 1280000\n",
    "        self.fc1 = nn.Linear(1280 * 1000, 1024)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(1024, 512)          # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(512, 3)             # Output layer for 3 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input\n",
    "        x = x.view(-1, 1280 * 1000)  # Ensure input tensor is reshaped to (batch_size, 1280*1000)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation function here as it will be used with CrossEntropyLoss\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def test_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Tests the given model on the provided test data loader.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The model to test.\n",
    "        test_loader (torch.utils.data.DataLoader): DataLoader for the test set.\n",
    "        device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on the test set: {accuracy:.2f}%')\n",
    "\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengchenfan/miniconda3/lib/python3.10/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=190\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.1616, Accuracy: 56.11%\n",
      "Accuracy of the model on the test set: 55.00%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLPClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)  # Add channel dimension\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            # print(total)\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "        accuracy = test_model(model, testdataloader, device)\n",
    "\n",
    "train(model, device, traindataloader, optimizer, criterion)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
