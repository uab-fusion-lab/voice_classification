{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T21:09:21.165369Z",
     "start_time": "2024-10-07T21:09:18.761531Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "labels_df = pd.read_excel('./data/NEW_IRB300012145_Patient_ID_deidentified.xlsx')\n",
    "Smokeing_status = labels_df.iloc[:, 4].to_list()\n",
    "\n",
    "Smokeing_status[149]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T21:09:21.479927Z",
     "start_time": "2024-10-07T21:09:21.170889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'never'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "class AmplitudeNormalization:\n",
    "    def __call__(self, waveform):\n",
    "        # Normalize the waveform to be within [-1, 1]\n",
    "        peak = waveform.abs().max()\n",
    "        if peak > 0:\n",
    "            waveform = waveform / peak\n",
    "        return waveform\n",
    "\n",
    "# To use it:\n",
    "# waveform, sample_rate = torchaudio.load('path/to/audio.wav')\n",
    "# waveform = AmplitudeNormalization()(waveform)\n",
    "\n",
    "\n",
    "class PadTrimAudio:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, waveform):\n",
    "        if waveform.size(1) > self.max_len:\n",
    "            # Trim the waveform if longer than max_len\n",
    "            waveform = waveform[:, :self.max_len]\n",
    "        elif waveform.size(1) < self.max_len:\n",
    "            # Pad with zeros if shorter than max_len\n",
    "            padding_size = self.max_len - waveform.size(1)\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding_size), \"constant\", 0)\n",
    "\n",
    "        if waveform.size(0) < 128:\n",
    "            padding_size = 128 - waveform.size(0)\n",
    "            waveform = torch.nn.functional.pad(waveform, (padding_size, 0), \"constant\", 0)\n",
    "        return waveform\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "class FeatureNormalization:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, features):\n",
    "        # Fit the scaler on the training set features\n",
    "        self.scaler.fit(features)\n",
    "\n",
    "    def transform(self, features):\n",
    "        # Apply normalization to features\n",
    "        return self.scaler.transform(features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T21:09:23.269555Z",
     "start_time": "2024-10-07T21:09:22.755510Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "label_to_int = {\n",
    "    'N': 0,\n",
    "    'Y': 1\n",
    "}\n",
    "\n",
    "label_to_int['N']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T19:52:49.433243Z",
     "start_time": "2024-10-07T19:52:49.414243Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "label_to_int = {\n",
    "    'F': 0,\n",
    "    'M': 1\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T19:52:50.268322Z",
     "start_time": "2024-10-07T19:52:50.250956Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "label_to_int = {\n",
    "    'current': 0,\n",
    "    'former': 0,\n",
    "    'never': 1\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T21:09:27.316500Z",
     "start_time": "2024-10-07T21:09:27.312938Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "class SoundDataset(Dataset):\n",
    "    def __init__(self, data_dir, labels_df, transform=None, max_len=1000):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels_df = labels_df\n",
    "        self.transform = transform\n",
    "        self.max_len = max_len\n",
    "        self.orderlist = ['RLP.wav', 'RUP.wav', 'RUA.wav', 'RMP.wav', 'LMP.wav', 'LUA.wav', 'LLP.wav', 'LUP.wav', 'cough_n.wav', 'two_roads.wav']\n",
    "        # self.orderlist_voice = ['a.wav', 'a_high.wav', 'a_low.wav', 'consent.wav', 'cough_n.wav', 'e.wav', 'e_high.wav', 'e_low.wav', 'pain_rating.wav', 'SOB_rating.wav', 'two_roads.wav']\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = str(idx+1).zfill(3)\n",
    "        audio_dir_breath = os.path.join(self.data_dir, str(patient_id), 'breath Eko')\n",
    "        audio_files = [os.path.join(audio_dir_breath, f) for f in os.listdir(audio_dir_breath) if f.endswith('.wav') and 'Hum' not in f]\n",
    "        \n",
    "        audio_dir_voice = os.path.join(self.data_dir, str(patient_id), 'Voice')\n",
    "        audio_file_tworoads  = [os.path.join(audio_dir_voice, f) for f in os.listdir(audio_dir_voice) if f.endswith('s.wav')][0]\n",
    "        audio_file_cough  = [os.path.join(audio_dir_voice, f) for f in os.listdir(audio_dir_voice) if f.endswith('n.wav')][0]\n",
    "        audio_files.append(audio_file_cough)\n",
    "        audio_files.append(audio_file_tworoads)\n",
    "\n",
    "        waveform_list = [torch.zeros(128, self.max_len) for _ in range(len(self.orderlist))]\n",
    "        for audio_file in audio_files:\n",
    "            file_name = os.path.basename(audio_file)\n",
    "            with open(audio_file, 'rb') as f:\n",
    "                index = next((i for i, item in enumerate(self.orderlist) if item in file_name), -1)\n",
    "    \n",
    "                y, sr = librosa.load(audio_file, sr=None)\n",
    "    \n",
    "                # STFT\n",
    "                D = librosa.stft(y, n_fft=2048, hop_length=512)  # n_fft and hop_length can be adjusted based on your needs\n",
    "                S = np.abs(D) ** 2  # Convert to power spectrum\n",
    "                mel_spec = librosa.feature.melspectrogram(S=S, sr=sr, n_mels=128)\n",
    "    \n",
    "                waveform = PadTrimAudio(self.max_len)(torch.tensor(mel_spec))\n",
    "                waveform_list[index] = waveform\n",
    "\n",
    "\n",
    "        # Concatenate all waveforms along the time dimension\n",
    "        waveform = torch.cat(waveform_list, dim=0)\n",
    "\n",
    "        # print(mel_spec)\n",
    "        # plt.figure(figsize=(10, 4))\n",
    "        # librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max), sr=sr, hop_length=512, x_axis='time', y_axis='mel')\n",
    "        # plt.colorbar(format='%+2.0f dB')\n",
    "        # plt.title('Mel spectrogram')\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "        # print(len(y))\n",
    "        # print(sr)\n",
    "\n",
    "\n",
    "\n",
    "        label = label_to_int[self.labels_df[idx]]\n",
    "\n",
    "        return waveform, label\n",
    "\n",
    "transform = AmplitudeNormalization()\n",
    "data_dir = './data/Patients'\n",
    "sound_dataset = SoundDataset(data_dir, Smokeing_status, transform=transform)\n",
    "\n",
    "train_dataset, test_dataset = random_split(sound_dataset, [180, 20])\n",
    "\n",
    "traindataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "testdataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "dataloader = DataLoader(sound_dataset, batch_size=20, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T21:18:02.542589Z",
     "start_time": "2024-10-07T21:18:02.504236Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "for batch in traindataloader:\n",
    "    waveforms, labels = batch\n",
    "    print(waveforms.shape)\n",
    "    print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T21:20:35.716145Z",
     "start_time": "2024-10-07T21:19:27.693616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1280, 1000])\n",
      "tensor([1, 1, 0, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([0, 1, 1, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([1, 0, 1, 0])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([1, 0, 1, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([1, 1, 1, 1])\n",
      "torch.Size([4, 1280, 1000])\n",
      "tensor([1, 0, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard\\AppData\\Local\\Temp\\ipykernel_32780\\3823968856.py:31: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(audio_file, sr=None)\n",
      "E:\\conda\\envs\\torchSoundEnv\\lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "ename": "NoBackendError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLibsndfileError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[1;32mE:\\conda\\envs\\torchSoundEnv\\lib\\site-packages\\librosa\\core\\audio.py:176\u001B[0m, in \u001B[0;36mload\u001B[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001B[0m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 176\u001B[0m     y, sr_native \u001B[38;5;241m=\u001B[39m \u001B[43m__soundfile_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mduration\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m sf\u001B[38;5;241m.\u001B[39mSoundFileRuntimeError \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    179\u001B[0m     \u001B[38;5;66;03m# If soundfile failed, try audioread instead\u001B[39;00m\n",
      "File \u001B[1;32mE:\\conda\\envs\\torchSoundEnv\\lib\\site-packages\\librosa\\core\\audio.py:209\u001B[0m, in \u001B[0;36m__soundfile_load\u001B[1;34m(path, offset, duration, dtype)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;66;03m# Otherwise, create the soundfile object\u001B[39;00m\n\u001B[1;32m--> 209\u001B[0m     context \u001B[38;5;241m=\u001B[39m \u001B[43msf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSoundFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    211\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context \u001B[38;5;28;01mas\u001B[39;00m sf_desc:\n",
      "File \u001B[1;32mE:\\conda\\envs\\torchSoundEnv\\lib\\site-packages\\soundfile.py:658\u001B[0m, in \u001B[0;36mSoundFile.__init__\u001B[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001B[0m\n\u001B[0;32m    656\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info \u001B[38;5;241m=\u001B[39m _create_info_struct(file, mode, samplerate, channels,\n\u001B[0;32m    657\u001B[0m                                  \u001B[38;5;28mformat\u001B[39m, subtype, endian)\n\u001B[1;32m--> 658\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode_int\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosefd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    659\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mset\u001B[39m(mode)\u001B[38;5;241m.\u001B[39missuperset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr+\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseekable():\n\u001B[0;32m    660\u001B[0m     \u001B[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001B[39;00m\n",
      "File \u001B[1;32mE:\\conda\\envs\\torchSoundEnv\\lib\\site-packages\\soundfile.py:1216\u001B[0m, in \u001B[0;36mSoundFile._open\u001B[1;34m(self, file, mode_int, closefd)\u001B[0m\n\u001B[0;32m   1215\u001B[0m     err \u001B[38;5;241m=\u001B[39m _snd\u001B[38;5;241m.\u001B[39msf_error(file_ptr)\n\u001B[1;32m-> 1216\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m LibsndfileError(err, prefix\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError opening \u001B[39m\u001B[38;5;132;01m{0!r}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname))\n\u001B[0;32m   1217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode_int \u001B[38;5;241m==\u001B[39m _snd\u001B[38;5;241m.\u001B[39mSFM_WRITE:\n\u001B[0;32m   1218\u001B[0m     \u001B[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001B[39;00m\n\u001B[0;32m   1219\u001B[0m     \u001B[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001B[39;00m\n\u001B[0;32m   1220\u001B[0m     \u001B[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001B[39;00m\n",
      "\u001B[1;31mLibsndfileError\u001B[0m: Error opening './data/Patients\\\\001\\\\breath Eko\\\\LUP.wav': Format not recognised.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mNoBackendError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m traindataloader:\n\u001B[0;32m      2\u001B[0m     waveforms, labels \u001B[38;5;241m=\u001B[39m batch\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(waveforms\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[1;32mE:\\conda\\envs\\torchSoundEnv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mE:\\conda\\envs\\torchSoundEnv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    671\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    672\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 673\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    674\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    675\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mE:\\conda\\envs\\torchSoundEnv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__getitems__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__:\n\u001B[1;32m---> 50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "File \u001B[1;32mE:\\conda\\envs\\torchSoundEnv\\lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001B[0m, in \u001B[0;36mSubset.__getitems__\u001B[1;34m(self, indices)\u001B[0m\n\u001B[0;32m    418\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m    419\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 420\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx]] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "File \u001B[1;32mE:\\conda\\envs\\torchSoundEnv\\lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    418\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m    419\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 420\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "Cell \u001B[1;32mIn[15], line 31\u001B[0m, in \u001B[0;36mSoundDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(audio_file, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m     29\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m((i \u001B[38;5;28;01mfor\u001B[39;00m i, item \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39morderlist) \u001B[38;5;28;01mif\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m file_name), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 31\u001B[0m     y, sr \u001B[38;5;241m=\u001B[39m \u001B[43mlibrosa\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;66;03m# STFT\u001B[39;00m\n\u001B[0;32m     34\u001B[0m     D \u001B[38;5;241m=\u001B[39m librosa\u001B[38;5;241m.\u001B[39mstft(y, n_fft\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2048\u001B[39m, hop_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m)  \u001B[38;5;66;03m# n_fft and hop_length can be adjusted based on your needs\u001B[39;00m\n",
      "File \u001B[1;32mE:\\conda\\envs\\torchSoundEnv\\lib\\site-packages\\librosa\\core\\audio.py:184\u001B[0m, in \u001B[0;36mload\u001B[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001B[0m\n\u001B[0;32m    180\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, (\u001B[38;5;28mstr\u001B[39m, pathlib\u001B[38;5;241m.\u001B[39mPurePath)):\n\u001B[0;32m    181\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    182\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPySoundFile failed. Trying audioread instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m\n\u001B[0;32m    183\u001B[0m     )\n\u001B[1;32m--> 184\u001B[0m     y, sr_native \u001B[38;5;241m=\u001B[39m \u001B[43m__audioread_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mduration\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    186\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "File \u001B[1;32mE:\\conda\\envs\\torchSoundEnv\\lib\\site-packages\\decorator.py:232\u001B[0m, in \u001B[0;36mdecorate.<locals>.fun\u001B[1;34m(*args, **kw)\u001B[0m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwsyntax:\n\u001B[0;32m    231\u001B[0m     args, kw \u001B[38;5;241m=\u001B[39m fix(args, kw, sig)\n\u001B[1;32m--> 232\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m caller(func, \u001B[38;5;241m*\u001B[39m(extras \u001B[38;5;241m+\u001B[39m args), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n",
      "File \u001B[1;32mE:\\conda\\envs\\torchSoundEnv\\lib\\site-packages\\librosa\\util\\decorators.py:59\u001B[0m, in \u001B[0;36mdeprecated.<locals>.__wrapper\u001B[1;34m(func, *args, **kwargs)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001B[39;00m\n\u001B[0;32m     51\u001B[0m warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m     52\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{:s}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{:s}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mDeprecated as of librosa version \u001B[39m\u001B[38;5;132;01m{:s}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mIt will be removed in librosa version \u001B[39m\u001B[38;5;132;01m{:s}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     57\u001B[0m     stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,  \u001B[38;5;66;03m# Would be 2, but the decorator adds a level\u001B[39;00m\n\u001B[0;32m     58\u001B[0m )\n\u001B[1;32m---> 59\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\conda\\envs\\torchSoundEnv\\lib\\site-packages\\librosa\\core\\audio.py:240\u001B[0m, in \u001B[0;36m__audioread_load\u001B[1;34m(path, offset, duration, dtype)\u001B[0m\n\u001B[0;32m    237\u001B[0m     reader \u001B[38;5;241m=\u001B[39m path\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    239\u001B[0m     \u001B[38;5;66;03m# If the input was not an audioread object, try to open it\u001B[39;00m\n\u001B[1;32m--> 240\u001B[0m     reader \u001B[38;5;241m=\u001B[39m \u001B[43maudioread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maudio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m reader \u001B[38;5;28;01mas\u001B[39;00m input_file:\n\u001B[0;32m    243\u001B[0m     sr_native \u001B[38;5;241m=\u001B[39m input_file\u001B[38;5;241m.\u001B[39msamplerate\n",
      "File \u001B[1;32mE:\\conda\\envs\\torchSoundEnv\\lib\\site-packages\\audioread\\__init__.py:132\u001B[0m, in \u001B[0;36maudio_open\u001B[1;34m(path, backends)\u001B[0m\n\u001B[0;32m    129\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;66;03m# All backends failed!\u001B[39;00m\n\u001B[1;32m--> 132\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m NoBackendError()\n",
      "\u001B[1;31mNoBackendError\u001B[0m: "
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "for batch in testdataloader:\n",
    "    waveforms, labels = batch\n",
    "    print(waveforms.shape)\n",
    "    print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T19:53:23.873587Z",
     "start_time": "2024-10-07T19:53:21.275932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128, 10000])\n",
      "tensor([1, 1, 0, 1])\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([1, 1, 1, 1])\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([1, 0, 1, 0])\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([1, 1, 0, 0])\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([0, 1, 0, 1])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        # Adjusted the number of input channels to 1280\n",
    "        self.conv1 = nn.Conv1d(128, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2, 2)\n",
    "        self.conv2 = nn.Conv1d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Calculate the size of the output from the last conv layer to pass to the first linear layer\n",
    "        initial_length = 10000  # Initial length of the signal\n",
    "        final_conv_length = self._get_conv1d_output_size(initial_length, 3, 1, 1, 2)  # After first conv and pool\n",
    "        final_conv_length = self._get_conv1d_output_size(final_conv_length, 3, 1, 1, 2)  # After second conv and pool\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * final_conv_length, 128)  # Linear layer for feature reduction\n",
    "        self.fc2 = nn.Linear(128, 2)  # Output layer for 3 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # Flatten from the second dimension onward\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def _get_conv1d_output_size(self, L, kernel_size, stride, padding, pool):\n",
    "        # Utility function to calculate the size of the output of a conv1d layer after pooling\n",
    "        L = (L + 2 * padding - kernel_size) // stride + 1\n",
    "        L = L // pool\n",
    "        return L\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        # Flatten the input from 1280x1000 to 1280000\n",
    "        self.fc1 = nn.Linear(1280 * 1000, 1024)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(1024, 512)          # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(512, 3)             # Output layer for 3 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input\n",
    "        x = x.view(-1, 1280 * 1000)  # Ensure input tensor is reshaped to (batch_size, 1280*1000)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation function here as it will be used with CrossEntropyLoss\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T19:53:27.127747Z",
     "start_time": "2024-10-07T19:53:27.116748Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def test_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Tests the given model on the provided test data loader.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The model to test.\n",
    "        test_loader (torch.utils.data.DataLoader): DataLoader for the test set.\n",
    "        device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            print(labels)\n",
    "            print(predicted)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on the test set: {accuracy:.2f}%')\n",
    "\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T19:53:30.122431Z",
     "start_time": "2024-10-07T19:53:30.113431Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AudioClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)  # Add channel dimension\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            # print(total)\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "        accuracy = test_model(model, testdataloader, device)\n",
    "\n",
    "train(model, device, traindataloader, optimizer, criterion)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T19:57:57.444726Z",
     "start_time": "2024-10-07T19:53:31.270121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.8651, Accuracy: 53.33%\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([1, 0, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([0, 1, 0, 1], device='cuda:0')\n",
      "tensor([0, 1, 1, 1], device='cuda:0')\n",
      "Accuracy of the model on the test set: 80.00%\n",
      "Epoch 2, Loss: 0.6166, Accuracy: 85.56%\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([0, 1, 0, 0], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([1, 1, 1, 0], device='cuda:0')\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([0, 1, 0, 1], device='cuda:0')\n",
      "tensor([0, 1, 1, 1], device='cuda:0')\n",
      "Accuracy of the model on the test set: 75.00%\n",
      "Epoch 3, Loss: 0.1699, Accuracy: 95.00%\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor([1, 0, 0, 1], device='cuda:0')\n",
      "tensor([0, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "Accuracy of the model on the test set: 70.00%\n",
      "Epoch 4, Loss: 0.0819, Accuracy: 98.89%\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 0, 0, 0], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor([1, 0, 0, 1], device='cuda:0')\n",
      "tensor([0, 1, 0, 1], device='cuda:0')\n",
      "tensor([0, 1, 1, 0], device='cuda:0')\n",
      "Accuracy of the model on the test set: 60.00%\n",
      "Epoch 5, Loss: 0.0184, Accuracy: 100.00%\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 0, 0, 0], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor([1, 0, 0, 1], device='cuda:0')\n",
      "tensor([0, 1, 0, 1], device='cuda:0')\n",
      "tensor([0, 1, 1, 0], device='cuda:0')\n",
      "Accuracy of the model on the test set: 60.00%\n",
      "Epoch 6, Loss: 0.0103, Accuracy: 100.00%\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 0, 0, 0], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([0, 1, 0, 1], device='cuda:0')\n",
      "tensor([0, 1, 1, 0], device='cuda:0')\n",
      "Accuracy of the model on the test set: 65.00%\n",
      "Epoch 7, Loss: 0.0063, Accuracy: 100.00%\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([0, 1, 0, 1], device='cuda:0')\n",
      "tensor([0, 1, 1, 0], device='cuda:0')\n",
      "Accuracy of the model on the test set: 75.00%\n",
      "Epoch 8, Loss: 0.0047, Accuracy: 100.00%\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([0, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 0], device='cuda:0')\n",
      "Accuracy of the model on the test set: 70.00%\n",
      "Epoch 9, Loss: 0.0036, Accuracy: 100.00%\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([0, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 0], device='cuda:0')\n",
      "Accuracy of the model on the test set: 70.00%\n",
      "Epoch 10, Loss: 0.0028, Accuracy: 100.00%\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([0, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 0], device='cuda:0')\n",
      "Accuracy of the model on the test set: 70.00%\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
