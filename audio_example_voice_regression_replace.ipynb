{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:23:14.057097900Z",
     "start_time": "2024-07-31T14:23:05.490176900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "[('020', 78.86),\n ('022', 83.72),\n ('023', 89.58),\n ('024', 96.51),\n ('025', 67.07),\n ('026', 73.86),\n ('029', 70.62),\n ('030', 89.22),\n ('031', 75.68),\n ('032', 78.21),\n ('033', 71.77),\n ('034', 60.95),\n ('035', 81.99),\n ('036', 80.05),\n ('037', 90.6),\n ('038', 82.59),\n ('039', 81.06),\n ('040', 81.5),\n ('041', 44.05),\n ('042', 77.16),\n ('043', 75.69),\n ('044', 82.62),\n ('045', 66.13),\n ('046', 76.57),\n ('047', 65.16),\n ('048', 63.78),\n ('049', 58.63),\n ('051', 68.89),\n ('052', 83.25),\n ('053', 85.91),\n ('054', 73.39),\n ('057', 55.0),\n ('058', 68.51),\n ('061', 88.75),\n ('063', 67.59),\n ('064', 61.47),\n ('065', 75.97),\n ('066', 69.39),\n ('067', 69.55),\n ('068', 83.77),\n ('069', 69.4),\n ('070', 33.99),\n ('073', 57.3),\n ('074', 79.12),\n ('075', 80.09),\n ('076', 75.58),\n ('078', 46.58),\n ('079', 76.7),\n ('080', 81.79),\n ('081', 87.53),\n ('082', 80.36),\n ('083', 91.9),\n ('084', 76.89),\n ('085', 79.3),\n ('088', 84.25),\n ('091', 63.78),\n ('093', 80.25),\n ('094', 87.95),\n ('095', 54.19),\n ('098', 47.47),\n ('099', 74.39),\n ('102', 79.16),\n ('104', 85.01),\n ('105', 37.17),\n ('107', 72.23),\n ('108', 72.98),\n ('109', 79.51),\n ('110', 80.98),\n ('111', 80.73),\n ('112', 81.23),\n ('113', 88.53),\n ('114', 79.3),\n ('115', 67.47),\n ('116', 79.87),\n ('117', 83.65),\n ('118', 87.59),\n ('119', 70.77),\n ('120', 43.94),\n ('121', 75.96),\n ('122', 88.79),\n ('123', 84.12),\n ('124', 76.72),\n ('125', 38.75),\n ('126', 79.01),\n ('127', 84.1),\n ('128', 74.88),\n ('129', 83.96),\n ('130', 91.31),\n ('131', 86.29),\n ('132', 83.71),\n ('133', 71.04),\n ('134', 84.7),\n ('135', 83.86),\n ('136', 84.66),\n ('137', 79.44),\n ('138', 83.32),\n ('139', 74.16),\n ('140', 75.72),\n ('141', 76.34),\n ('142', 74.05),\n ('143', 71.86),\n ('145', 75.5),\n ('147', 82.16),\n ('149', 84.78),\n ('150', 84.44),\n ('151', 62.84),\n ('152', 80.2),\n ('153', 67.72),\n ('154', 69.11),\n ('155', 86.15),\n ('156', 76.89),\n ('157', 28.3),\n ('158', 75.39),\n ('159', 54.08),\n ('161', 71.27),\n ('162', 78.81),\n ('163', 92.22),\n ('164', 73.08),\n ('165', 94.61),\n ('166', 81.5),\n ('167', 76.91),\n ('168', 30.88),\n ('170', 79.58),\n ('171', 71.88),\n ('172', 73.29),\n ('173', 79.33),\n ('174', 77.98),\n ('175', 51.3),\n ('176', 81.99),\n ('177', 59.0),\n ('178', 39.1),\n ('179', 62.9),\n ('180', 46.51),\n ('181', 74.79),\n ('182', 77.93),\n ('183', 81.45),\n ('184', 80.3),\n ('185', 93.33),\n ('186', 79.51),\n ('187', 78.15),\n ('188', 79.05),\n ('189', 81.27),\n ('190', 84.72),\n ('192', 59.59),\n ('193', 69.87),\n ('194', 62.54),\n ('195', 81.78),\n ('196', 76.58),\n ('197', 75.92),\n ('198', 46.35),\n ('199', 78.27),\n ('200', 92.57)]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = pd.read_excel('./data/data/NEW_IRB300012145_Patient_ID_deidentified.xlsx')\n",
    "smoking_status = labels_df.iloc[:, [0,27]]\n",
    "smoking_status.iloc[:, 0] = smoking_status.iloc[:, 0].apply(lambda x: x[-3:])\n",
    "\n",
    "filtered_df = smoking_status[~pd.isna(smoking_status.iloc[:, 1])]\n",
    "\n",
    "label_list = list(filtered_df.itertuples(index=False, name=None))\n",
    "label_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:24:59.554726700Z",
     "start_time": "2024-07-31T14:24:58.868666700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "class AmplitudeNormalization:\n",
    "    def __call__(self, waveform):\n",
    "        # Normalize the waveform to be within [-1, 1]\n",
    "        peak = waveform.abs().max()\n",
    "        if peak > 0:\n",
    "            waveform = waveform / peak\n",
    "        return waveform\n",
    "\n",
    "# To use it:\n",
    "# waveform, sample_rate = torchaudio.load('path/to/audio.wav')\n",
    "# waveform = AmplitudeNormalization()(waveform)\n",
    "\n",
    "\n",
    "class PadTrimAudio:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, waveform):\n",
    "        if waveform.size(1) > self.max_len:\n",
    "            # Trim the waveform if longer than max_len\n",
    "            waveform = waveform[:, :self.max_len]\n",
    "        elif waveform.size(1) < self.max_len:\n",
    "            # Pad with zeros if shorter than max_len\n",
    "            padding_size = self.max_len - waveform.size(1)\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding_size), \"constant\", 0)\n",
    "\n",
    "        if waveform.size(0) < 128:\n",
    "            padding_size = 128 - waveform.size(0)\n",
    "            waveform = torch.nn.functional.pad(waveform, (padding_size, 0), \"constant\", 0)\n",
    "        return waveform\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "class FeatureNormalization:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, features):\n",
    "        # Fit the scaler on the training set features\n",
    "        self.scaler.fit(features)\n",
    "\n",
    "    def transform(self, features):\n",
    "        # Apply normalization to features\n",
    "        return self.scaler.transform(features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:25:03.896744700Z",
     "start_time": "2024-07-31T14:25:01.143009300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "label_to_int = {\n",
    "    'Normal': 0,\n",
    "    'Restrictive vent. Defect': 1,\n",
    "    'Obstructive vent. Defect': 1,\n",
    "    'Obstr. Cannot R/O Restriction': 1\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class SoundDataset(Dataset):\n",
    "    def __init__(self, data_dir, labels_df, transform=None, max_len=10000):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels_df = labels_df\n",
    "        self.transform = transform\n",
    "        self.max_len = max_len\n",
    "        self.orderlist = ['RLP.wav', 'RUP.wav', 'RUA Hum.wav', 'LUA Hum.wav', 'RUA.wav', 'RMP.wav', 'LMP.wav', 'LUA.wav', 'LLP.wav', 'LUP.wav']\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.labels_df[idx][0]\n",
    "        # audio_dir = os.path.join(self.data_dir, str(patient_id), 'breath Eko')\n",
    "        audio_dir = os.path.join(self.data_dir, str(patient_id), 'Voice')\n",
    "        audio_file = [os.path.join(audio_dir, f) for f in os.listdir(audio_dir) if f.endswith('.wav') and '_' not in f][0]\n",
    "\n",
    "        y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "\n",
    "        D = librosa.stft(y, n_fft=2048, hop_length=512)  # n_fft and hop_length can be adjusted based on your needs\n",
    "        S = np.abs(D) ** 2  # Convert to power spectrum\n",
    "        mel_spec = librosa.feature.melspectrogram(S=S, sr=sr, n_mels=128)\n",
    "\n",
    "\n",
    "        waveform = PadTrimAudio(self.max_len)(torch.tensor(mel_spec))\n",
    "\n",
    "        label = self.labels_df[idx][1]\n",
    "\n",
    "        return waveform, label\n",
    "\n",
    "transform = AmplitudeNormalization()\n",
    "data_dir = './data/data/Patients'\n",
    "sound_dataset = SoundDataset(data_dir, label_list, transform=transform)\n",
    "\n",
    "all_len =  len(label_list)\n",
    "train_len = round(all_len * 0.9)\n",
    "\n",
    "train_dataset, test_dataset = random_split(sound_dataset, [train_len, all_len-train_len])\n",
    "\n",
    "traindataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "testdataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "dataloader = DataLoader(sound_dataset, batch_size=20, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:32:20.334561400Z",
     "start_time": "2024-07-31T14:32:20.325243100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128, 10000])\n",
      "tensor([62.5400, 67.5900, 71.7700, 73.0800], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([82.6200, 65.1600, 62.9000, 83.7200], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([44.0500, 75.9200, 71.2700, 86.2900], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([67.0700, 74.1600, 81.2300, 89.5800], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([38.7500, 83.7700, 96.5100, 83.8600], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([46.3500, 89.2200, 79.1600, 74.0500], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([84.1000, 81.5000, 79.1200, 80.3600], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([67.4700, 83.7100, 80.0500, 70.7700], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([73.8600, 79.5100, 70.6200, 88.5300], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([79.0500, 79.4400, 93.3300, 75.6800], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([84.7200, 80.9800, 76.7200, 81.4500], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([78.1500, 76.8900, 66.1300, 90.6000], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([84.2500, 75.5800, 37.1700, 30.8800], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([80.0900, 77.9300, 76.7000, 85.9100], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([75.5000, 79.5800, 54.0800, 69.4000], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([82.5900, 69.5500, 75.9600, 59.0000], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([28.3000, 75.6900, 83.3200, 67.7200], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([80.2000, 83.6500, 79.3000, 43.9400], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([87.9500, 81.9900, 71.0400, 69.1100], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([81.2700, 74.7900, 80.7300, 33.9900], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([73.2900, 81.9900, 54.1900, 46.5100], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([83.2500, 71.8800, 46.5800, 78.2100], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([81.7900, 78.8600, 74.8800, 84.7800], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([92.5700, 57.3000, 80.2500, 79.8700], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([73.3900, 75.7200, 88.7900, 61.4700], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([78.2700, 72.9800, 87.5300, 77.9800], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([84.4400, 79.3300, 74.3900, 82.1600], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([91.3100, 68.5100, 91.9000, 79.0100], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([60.9500, 94.6100, 69.8700, 75.9700], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([76.5700, 62.8400, 84.1200, 79.3000], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([72.2300, 79.5100, 55.0000, 78.8100], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([77.1600, 83.9600, 51.3000, 92.2200], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([75.3900, 86.1500, 88.7500, 76.8900], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([76.3400, 81.5000, 63.7800, 80.3000], dtype=torch.float64)\n",
      "torch.Size([1, 128, 10000])\n",
      "tensor([69.3900], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for batch in traindataloader:\n",
    "    waveforms, labels = batch\n",
    "    print(waveforms.shape)\n",
    "    print(labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128, 10000])\n",
      "tensor([58.6300, 63.7800, 76.9100, 85.0100], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([47.4700, 87.5900, 68.8900, 76.5800], dtype=torch.float64)\n",
      "torch.Size([4, 128, 10000])\n",
      "tensor([84.7000, 59.5900, 39.1000, 71.8600], dtype=torch.float64)\n",
      "torch.Size([3, 128, 10000])\n",
      "tensor([81.7800, 81.0600, 84.6600], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for batch in testdataloader:\n",
    "    waveforms, labels = batch\n",
    "    print(waveforms.shape)\n",
    "    print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:32:46.486415500Z",
     "start_time": "2024-07-31T14:32:44.989845200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        # Adjusted the number of input channels to 1280\n",
    "        self.conv1 = nn.Conv1d(128, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2, 2)\n",
    "        self.conv2 = nn.Conv1d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Calculate the size of the output from the last conv layer to pass to the first linear layer\n",
    "        initial_length = 10000  # Initial length of the signal\n",
    "        final_conv_length = self._get_conv1d_output_size(initial_length, 3, 1, 1, 2)  # After first conv and pool\n",
    "        final_conv_length = self._get_conv1d_output_size(final_conv_length, 3, 1, 1, 2)  # After second conv and pool\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * final_conv_length, 128)  # Linear layer for feature reduction\n",
    "        self.fc2 = nn.Linear(128, 1)  # Output layer for 3 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # Flatten from the second dimension onward\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def _get_conv1d_output_size(self, L, kernel_size, stride, padding, pool):\n",
    "        # Utility function to calculate the size of the output of a conv1d layer after pooling\n",
    "        L = (L + 2 * padding - kernel_size) // stride + 1\n",
    "        L = L // pool\n",
    "        return L\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        # Flatten the input from 1280x1000 to 1280000\n",
    "        self.fc1 = nn.Linear(1280 * 1000, 1024)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(1024, 512)          # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(512, 3)             # Output layer for 3 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input\n",
    "        x = x.view(-1, 1280 * 1000)  # Ensure input tensor is reshaped to (batch_size, 1280*1000)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation function here as it will be used with CrossEntropyLoss\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:35:42.434587900Z",
     "start_time": "2024-07-31T14:35:42.429998100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def test_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Tests the given model on the provided test data loader.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The model to test.\n",
    "        test_loader (torch.utils.data.DataLoader): DataLoader for the test set.\n",
    "        device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_mse = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            print(labels)\n",
    "            print(outputs.squeeze())\n",
    "            criterion = nn.MSELoss()\n",
    "            all_mse += criterion(outputs.squeeze(), labels)\n",
    "            print('----------------------------------------------')\n",
    "        print(all_mse)\n",
    "\n",
    "\n",
    "    accuracy = 0\n",
    "\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:35:43.742251500Z",
     "start_time": "2024-07-31T14:35:43.734089800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10022.9601\n",
      "tensor([58.6300, 63.7800, 76.9100, 85.0100], dtype=torch.float64)\n",
      "tensor([ 56.9032, 218.8469,  33.0102,   7.7583])\n",
      "----------------------------------------------\n",
      "tensor([47.4700, 87.5900, 68.8900, 76.5800], dtype=torch.float64)\n",
      "tensor([101.5140,  48.6252,  50.2133,  10.6333])\n",
      "----------------------------------------------\n",
      "tensor([84.7000, 59.5900, 39.1000, 71.8600], dtype=torch.float64)\n",
      "tensor([64.1892, 36.2095, 29.6432, 53.2128])\n",
      "----------------------------------------------\n",
      "tensor([81.7800, 81.0600, 84.6600], dtype=torch.float64)\n",
      "tensor([44.2152, 34.3144,  9.8557])\n",
      "----------------------------------------------\n",
      "tensor(13685.2457, dtype=torch.float64)\n",
      "Epoch 2, Loss: 2435.0509\n",
      "tensor([58.6300, 63.7800, 76.9100, 85.0100], dtype=torch.float64)\n",
      "tensor([102.2732, 128.0534,  28.8913,  14.1675])\n",
      "----------------------------------------------\n",
      "tensor([47.4700, 87.5900, 68.8900, 76.5800], dtype=torch.float64)\n",
      "tensor([96.5901, 51.7649, 61.4852, 17.5850])\n",
      "----------------------------------------------\n",
      "tensor([84.7000, 59.5900, 39.1000, 71.8600], dtype=torch.float64)\n",
      "tensor([78.3596, 36.3862, 27.9775, 60.4791])\n",
      "----------------------------------------------\n",
      "tensor([81.7800, 81.0600, 84.6600], dtype=torch.float64)\n",
      "tensor([45.1674, 45.2497, 14.6524])\n",
      "----------------------------------------------\n",
      "tensor(7863.8648, dtype=torch.float64)\n",
      "Epoch 3, Loss: 1123.5248\n",
      "tensor([58.6300, 63.7800, 76.9100, 85.0100], dtype=torch.float64)\n",
      "tensor([124.2487, 112.3393,  33.3427,  18.0746])\n",
      "----------------------------------------------\n",
      "tensor([47.4700, 87.5900, 68.8900, 76.5800], dtype=torch.float64)\n",
      "tensor([78.8307, 53.7960, 83.2338, 22.4164])\n",
      "----------------------------------------------\n",
      "tensor([84.7000, 59.5900, 39.1000, 71.8600], dtype=torch.float64)\n",
      "tensor([79.1543, 39.5480, 43.3940, 69.1505])\n",
      "----------------------------------------------\n",
      "tensor([81.7800, 81.0600, 84.6600], dtype=torch.float64)\n",
      "tensor([52.7897, 43.5392, 17.8807])\n",
      "----------------------------------------------\n",
      "tensor(6927.2719, dtype=torch.float64)\n",
      "Epoch 4, Loss: 544.8230\n",
      "tensor([58.6300, 63.7800, 76.9100, 85.0100], dtype=torch.float64)\n",
      "tensor([151.5898, 135.2209,  40.6312,  23.9562])\n",
      "----------------------------------------------\n",
      "tensor([47.4700, 87.5900, 68.8900, 76.5800], dtype=torch.float64)\n",
      "tensor([97.0722, 59.7992, 88.6947, 26.7569])\n",
      "----------------------------------------------\n",
      "tensor([84.7000, 59.5900, 39.1000, 71.8600], dtype=torch.float64)\n",
      "tensor([91.2359, 45.4150, 46.0995, 67.5332])\n",
      "----------------------------------------------\n",
      "tensor([81.7800, 81.0600, 84.6600], dtype=torch.float64)\n",
      "tensor([63.5337, 57.7198, 22.8308])\n",
      "----------------------------------------------\n",
      "tensor(7868.7653, dtype=torch.float64)\n",
      "Epoch 5, Loss: 325.4611\n",
      "tensor([58.6300, 63.7800, 76.9100, 85.0100], dtype=torch.float64)\n",
      "tensor([165.9843, 144.0343,  40.4984,  29.6191])\n",
      "----------------------------------------------\n",
      "tensor([47.4700, 87.5900, 68.8900, 76.5800], dtype=torch.float64)\n",
      "tensor([97.1162, 64.9248, 94.9290, 33.5568])\n",
      "----------------------------------------------\n",
      "tensor([84.7000, 59.5900, 39.1000, 71.8600], dtype=torch.float64)\n",
      "tensor([93.2714, 52.1694, 47.3948, 65.9211])\n",
      "----------------------------------------------\n",
      "tensor([81.7800, 81.0600, 84.6600], dtype=torch.float64)\n",
      "tensor([68.9370, 61.1011, 28.4116])\n",
      "----------------------------------------------\n",
      "tensor(8267.3320, dtype=torch.float64)\n",
      "Epoch 6, Loss: 166.1880\n",
      "tensor([58.6300, 63.7800, 76.9100, 85.0100], dtype=torch.float64)\n",
      "tensor([189.9808, 149.4072,  43.9184,  34.5138])\n",
      "----------------------------------------------\n",
      "tensor([47.4700, 87.5900, 68.8900, 76.5800], dtype=torch.float64)\n",
      "tensor([100.6858,  69.6721,  95.9697,  38.4230])\n",
      "----------------------------------------------\n",
      "tensor([84.7000, 59.5900, 39.1000, 71.8600], dtype=torch.float64)\n",
      "tensor([97.7997, 56.2691, 50.6665, 65.0406])\n",
      "----------------------------------------------\n",
      "tensor([81.7800, 81.0600, 84.6600], dtype=torch.float64)\n",
      "tensor([71.2247, 69.2765, 33.1551])\n",
      "----------------------------------------------\n",
      "tensor(9449.8025, dtype=torch.float64)\n",
      "Epoch 7, Loss: 105.9662\n",
      "tensor([58.6300, 63.7800, 76.9100, 85.0100], dtype=torch.float64)\n",
      "tensor([205.2326, 158.6248,  49.1232,  39.4486])\n",
      "----------------------------------------------\n",
      "tensor([47.4700, 87.5900, 68.8900, 76.5800], dtype=torch.float64)\n",
      "tensor([104.9729,  74.8750, 104.0596,  43.8130])\n",
      "----------------------------------------------\n",
      "tensor([84.7000, 59.5900, 39.1000, 71.8600], dtype=torch.float64)\n",
      "tensor([103.3174,  59.5885,  54.6165,  73.7191])\n",
      "----------------------------------------------\n",
      "tensor([81.7800, 81.0600, 84.6600], dtype=torch.float64)\n",
      "tensor([76.5137, 76.4510, 37.8952])\n",
      "----------------------------------------------\n",
      "tensor(10671.6762, dtype=torch.float64)\n",
      "Epoch 8, Loss: 71.7469\n",
      "tensor([58.6300, 63.7800, 76.9100, 85.0100], dtype=torch.float64)\n",
      "tensor([210.9405, 156.4044,  50.6244,  42.9075])\n",
      "----------------------------------------------\n",
      "tensor([47.4700, 87.5900, 68.8900, 76.5800], dtype=torch.float64)\n",
      "tensor([104.0185,  76.7598, 100.8723,  47.5929])\n",
      "----------------------------------------------\n",
      "tensor([84.7000, 59.5900, 39.1000, 71.8600], dtype=torch.float64)\n",
      "tensor([104.2502,  60.8552,  57.1978,  74.6862])\n",
      "----------------------------------------------\n",
      "tensor([81.7800, 81.0600, 84.6600], dtype=torch.float64)\n",
      "tensor([77.2650, 78.6423, 41.2908])\n",
      "----------------------------------------------\n",
      "tensor(10670.4065, dtype=torch.float64)\n",
      "Epoch 9, Loss: 53.9733\n",
      "tensor([58.6300, 63.7800, 76.9100, 85.0100], dtype=torch.float64)\n",
      "tensor([197.8742, 142.7761,  48.7742,  42.7932])\n",
      "----------------------------------------------\n",
      "tensor([47.4700, 87.5900, 68.8900, 76.5800], dtype=torch.float64)\n",
      "tensor([95.5045, 73.1768, 94.5189, 47.0279])\n",
      "----------------------------------------------\n",
      "tensor([84.7000, 59.5900, 39.1000, 71.8600], dtype=torch.float64)\n",
      "tensor([95.8922, 56.8633, 54.1097, 70.3247])\n",
      "----------------------------------------------\n",
      "tensor([81.7800, 81.0600, 84.6600], dtype=torch.float64)\n",
      "tensor([72.8433, 75.9894, 40.9064])\n",
      "----------------------------------------------\n",
      "tensor(8825.5145, dtype=torch.float64)\n",
      "Epoch 10, Loss: 50.6098\n",
      "tensor([58.6300, 63.7800, 76.9100, 85.0100], dtype=torch.float64)\n",
      "tensor([214.9179, 154.3395,  51.5779,  46.5265])\n",
      "----------------------------------------------\n",
      "tensor([47.4700, 87.5900, 68.8900, 76.5800], dtype=torch.float64)\n",
      "tensor([99.5437, 77.7020, 94.7468, 51.3085])\n",
      "----------------------------------------------\n",
      "tensor([84.7000, 59.5900, 39.1000, 71.8600], dtype=torch.float64)\n",
      "tensor([101.2609,  60.6530,  56.6785,  75.1793])\n",
      "----------------------------------------------\n",
      "tensor([81.7800, 81.0600, 84.6600], dtype=torch.float64)\n",
      "tensor([76.8816, 83.4562, 44.8638])\n",
      "----------------------------------------------\n",
      "tensor(10403.2457, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AudioClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            data = data.to(device).float()  # Convert data to float\n",
    "            targets = targets.to(device).float()  # Convert targets to float\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)  # Add channel dimension\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')\n",
    "        accuracy = test_model(model, testdataloader, device)\n",
    "\n",
    "train(model, device, traindataloader, optimizer, criterion)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:38:15.048564300Z",
     "start_time": "2024-07-31T14:35:46.996449300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
