{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:23:14.057097900Z",
     "start_time": "2024-07-31T14:23:05.490176900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "[('020', 78.86),\n ('022', 83.72),\n ('023', 89.58),\n ('024', 96.51),\n ('025', 67.07),\n ('026', 73.86),\n ('029', 70.62),\n ('030', 89.22),\n ('031', 75.68),\n ('032', 78.21),\n ('033', 71.77),\n ('034', 60.95),\n ('035', 81.99),\n ('036', 80.05),\n ('037', 90.6),\n ('038', 82.59),\n ('039', 81.06),\n ('040', 81.5),\n ('041', 44.05),\n ('042', 77.16),\n ('043', 75.69),\n ('044', 82.62),\n ('045', 66.13),\n ('046', 76.57),\n ('047', 65.16),\n ('048', 63.78),\n ('049', 58.63),\n ('051', 68.89),\n ('052', 83.25),\n ('053', 85.91),\n ('054', 73.39),\n ('057', 55.0),\n ('058', 68.51),\n ('061', 88.75),\n ('063', 67.59),\n ('064', 61.47),\n ('065', 75.97),\n ('066', 69.39),\n ('067', 69.55),\n ('068', 83.77),\n ('069', 69.4),\n ('070', 33.99),\n ('073', 57.3),\n ('074', 79.12),\n ('075', 80.09),\n ('076', 75.58),\n ('078', 46.58),\n ('079', 76.7),\n ('080', 81.79),\n ('081', 87.53),\n ('082', 80.36),\n ('083', 91.9),\n ('084', 76.89),\n ('085', 79.3),\n ('088', 84.25),\n ('091', 63.78),\n ('093', 80.25),\n ('094', 87.95),\n ('095', 54.19),\n ('098', 47.47),\n ('099', 74.39),\n ('102', 79.16),\n ('104', 85.01),\n ('105', 37.17),\n ('107', 72.23),\n ('108', 72.98),\n ('109', 79.51),\n ('110', 80.98),\n ('111', 80.73),\n ('112', 81.23),\n ('113', 88.53),\n ('114', 79.3),\n ('115', 67.47),\n ('116', 79.87),\n ('117', 83.65),\n ('118', 87.59),\n ('119', 70.77),\n ('120', 43.94),\n ('121', 75.96),\n ('122', 88.79),\n ('123', 84.12),\n ('124', 76.72),\n ('125', 38.75),\n ('126', 79.01),\n ('127', 84.1),\n ('128', 74.88),\n ('129', 83.96),\n ('130', 91.31),\n ('131', 86.29),\n ('132', 83.71),\n ('133', 71.04),\n ('134', 84.7),\n ('135', 83.86),\n ('136', 84.66),\n ('137', 79.44),\n ('138', 83.32),\n ('139', 74.16),\n ('140', 75.72),\n ('141', 76.34),\n ('142', 74.05),\n ('143', 71.86),\n ('145', 75.5),\n ('147', 82.16),\n ('149', 84.78),\n ('150', 84.44),\n ('151', 62.84),\n ('152', 80.2),\n ('153', 67.72),\n ('154', 69.11),\n ('155', 86.15),\n ('156', 76.89),\n ('157', 28.3),\n ('158', 75.39),\n ('159', 54.08),\n ('161', 71.27),\n ('162', 78.81),\n ('163', 92.22),\n ('164', 73.08),\n ('165', 94.61),\n ('166', 81.5),\n ('167', 76.91),\n ('168', 30.88),\n ('170', 79.58),\n ('171', 71.88),\n ('172', 73.29),\n ('173', 79.33),\n ('174', 77.98),\n ('175', 51.3),\n ('176', 81.99),\n ('177', 59.0),\n ('178', 39.1),\n ('179', 62.9),\n ('180', 46.51),\n ('181', 74.79),\n ('182', 77.93),\n ('183', 81.45),\n ('184', 80.3),\n ('185', 93.33),\n ('186', 79.51),\n ('187', 78.15),\n ('188', 79.05),\n ('189', 81.27),\n ('190', 84.72),\n ('192', 59.59),\n ('193', 69.87),\n ('194', 62.54),\n ('195', 81.78),\n ('196', 76.58),\n ('197', 75.92),\n ('198', 46.35),\n ('199', 78.27),\n ('200', 92.57)]"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = pd.read_excel('./data/data/NEW_IRB300012145_Patient_ID_deidentified.xlsx')\n",
    "smoking_status = labels_df.iloc[:, [0,27]]\n",
    "smoking_status.iloc[:, 0] = smoking_status.iloc[:, 0].apply(lambda x: x[-3:])\n",
    "\n",
    "filtered_df = smoking_status[~pd.isna(smoking_status.iloc[:, 1])]\n",
    "\n",
    "label_list = list(filtered_df.itertuples(index=False, name=None))\n",
    "label_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:24:59.554726700Z",
     "start_time": "2024-07-31T14:24:58.868666700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "class AmplitudeNormalization:\n",
    "    def __call__(self, waveform):\n",
    "        # Normalize the waveform to be within [-1, 1]\n",
    "        peak = waveform.abs().max()\n",
    "        if peak > 0:\n",
    "            waveform = waveform / peak\n",
    "        return waveform\n",
    "\n",
    "# To use it:\n",
    "# waveform, sample_rate = torchaudio.load('path/to/audio.wav')\n",
    "# waveform = AmplitudeNormalization()(waveform)\n",
    "\n",
    "\n",
    "class PadTrimAudio:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, waveform):\n",
    "        if waveform.size(1) > self.max_len:\n",
    "            # Trim the waveform if longer than max_len\n",
    "            waveform = waveform[:, :self.max_len]\n",
    "        elif waveform.size(1) < self.max_len:\n",
    "            # Pad with zeros if shorter than max_len\n",
    "            padding_size = self.max_len - waveform.size(1)\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding_size), \"constant\", 0)\n",
    "\n",
    "        if waveform.size(0) < 128:\n",
    "            padding_size = 128 - waveform.size(0)\n",
    "            waveform = torch.nn.functional.pad(waveform, (padding_size, 0), \"constant\", 0)\n",
    "        return waveform\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "class FeatureNormalization:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, features):\n",
    "        # Fit the scaler on the training set features\n",
    "        self.scaler.fit(features)\n",
    "\n",
    "    def transform(self, features):\n",
    "        # Apply normalization to features\n",
    "        return self.scaler.transform(features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:25:03.896744700Z",
     "start_time": "2024-07-31T14:25:01.143009300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "label_to_int = {\n",
    "    'Normal': 0,\n",
    "    'Restrictive vent. Defect': 1,\n",
    "    'Obstructive vent. Defect': 1,\n",
    "    'Obstr. Cannot R/O Restriction': 1\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "class SoundDataset(Dataset):\n",
    "    def __init__(self, data_dir, labels_df, transform=None, max_len=500):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels_df = labels_df\n",
    "        self.transform = transform\n",
    "        self.max_len = max_len\n",
    "        self.orderlist = ['RLP.wav', 'RUP.wav', 'RUA Hum.wav', 'LUA Hum.wav', 'RUA.wav', 'RMP.wav', 'LMP.wav', 'LUA.wav', 'LLP.wav', 'LUP.wav']\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.labels_df[idx][0]\n",
    "            # audio_dir = os.path.join(self.data_dir, str(patient_id), 'breath Eko')\n",
    "        audio_dir = os.path.join(self.data_dir, str(patient_id), 'Voice')\n",
    "        audio_file = [os.path.join(audio_dir, f) for f in os.listdir(audio_dir) if f.endswith('.wav') and 'cough' in f][0]\n",
    "\n",
    "        # Concatenate audio files\n",
    "        y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "\n",
    "        D = librosa.stft(y, n_fft=2048, hop_length=512)  # n_fft and hop_length can be adjusted based on your needs\n",
    "        S = np.abs(D) ** 2  # Convert to power spectrum\n",
    "        mel_spec = librosa.feature.melspectrogram(S=S, sr=sr, n_mels=128)\n",
    "\n",
    "        waveform = PadTrimAudio(self.max_len)(torch.tensor(mel_spec))\n",
    "\n",
    "\n",
    "        label = self.labels_df[idx][1]\n",
    "\n",
    "        return waveform, label\n",
    "\n",
    "transform = AmplitudeNormalization()\n",
    "data_dir = './data/data/Patients'\n",
    "sound_dataset = SoundDataset(data_dir, label_list, transform=transform)\n",
    "\n",
    "all_len =  len(label_list)\n",
    "train_len = round(all_len * 0.9)\n",
    "\n",
    "train_dataset, test_dataset = random_split(sound_dataset, [train_len, all_len-train_len])\n",
    "\n",
    "traindataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "testdataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "dataloader = DataLoader(sound_dataset, batch_size=20, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:32:20.334561400Z",
     "start_time": "2024-07-31T14:32:20.325243100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128, 500])\n",
      "tensor([74.7900, 60.9500, 87.5900, 71.7700], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([79.0500, 69.4000, 83.3200, 71.8600], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([81.0600, 86.2900, 72.9800, 82.5900], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([80.0900, 71.8800, 69.8700, 76.7200], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([75.5800, 75.9600, 81.4500, 82.6200], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([75.9200, 75.7200, 73.0800, 39.1000], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([78.8600, 69.3900, 44.0500, 81.2300], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([76.5700, 84.7000, 89.5800, 55.0000], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([79.1200, 62.5400, 75.5000, 81.5000], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([82.1600, 83.6500, 61.4700, 96.5100], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([37.1700, 46.5800, 54.0800, 88.7500], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([92.2200, 78.2100, 38.7500, 81.9900], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([80.9800, 57.3000, 46.5100, 88.7900], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([75.9700, 76.3400, 63.7800, 63.7800], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([84.1000, 71.2700, 89.2200, 73.3900], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([84.1200, 54.1900, 93.3300, 85.0100], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([90.6000, 80.3600, 83.7700, 83.8600], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([65.1600, 33.9900, 79.4400, 81.2700], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([75.6900, 80.2500, 76.9100, 59.5900], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([80.2000, 84.7800, 81.9900, 72.2300], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([79.5100, 67.5900, 70.6200, 46.3500], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([74.8800, 83.7100, 77.9800, 67.0700], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([91.9000, 67.4700, 58.6300, 83.2500], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([71.0400, 77.9300, 84.4400, 79.3300], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([73.2900, 43.9400, 86.1500, 62.9000], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([67.7200, 69.1100, 68.8900, 74.3900], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([70.7700, 94.6100, 76.5800, 62.8400], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([80.3000, 75.6800, 81.5000, 79.5800], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([47.4700, 28.3000, 76.8900, 30.8800], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([74.1600, 79.8700, 77.1600, 78.1500], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([66.1300, 76.7000, 81.7800, 79.3000], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([84.7200, 59.0000, 76.8900, 78.2700], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([74.0500, 85.9100, 69.5500, 78.8100], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([79.5100, 79.0100, 81.7900, 79.1600], dtype=torch.float64)\n",
      "torch.Size([1, 128, 500])\n",
      "tensor([92.5700], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for batch in traindataloader:\n",
    "    waveforms, labels = batch\n",
    "    print(waveforms.shape)\n",
    "    print(labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128, 500])\n",
      "tensor([91.3100, 83.7200, 87.5300, 84.6600], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([79.3000, 80.7300, 80.0500, 83.9600], dtype=torch.float64)\n",
      "torch.Size([4, 128, 500])\n",
      "tensor([73.8600, 68.5100, 51.3000, 87.9500], dtype=torch.float64)\n",
      "torch.Size([3, 128, 500])\n",
      "tensor([84.2500, 75.3900, 88.5300], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for batch in testdataloader:\n",
    "    waveforms, labels = batch\n",
    "    print(waveforms.shape)\n",
    "    print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:32:46.486415500Z",
     "start_time": "2024-07-31T14:32:44.989845200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        # Adjusted the number of input channels to 1280\n",
    "        self.conv1 = nn.Conv1d(128, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2, 2)\n",
    "        self.conv2 = nn.Conv1d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Calculate the size of the output from the last conv layer to pass to the first linear layer\n",
    "        initial_length = 500  # Initial length of the signal\n",
    "        final_conv_length = self._get_conv1d_output_size(initial_length, 3, 1, 1, 2)  # After first conv and pool\n",
    "        final_conv_length = self._get_conv1d_output_size(final_conv_length, 3, 1, 1, 2)  # After second conv and pool\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * final_conv_length, 128)   # Linear layer for feature reduction\n",
    "        self.fc2 = nn.Linear(128, 1)  # Output layer for 3 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # Flatten from the second dimension onward\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def _get_conv1d_output_size(self, L, kernel_size, stride, padding, pool):\n",
    "        # Utility function to calculate the size of the output of a conv1d layer after pooling\n",
    "        L = (L + 2 * padding - kernel_size) // stride + 1\n",
    "        L = L // pool\n",
    "        return L\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        # Flatten the input from 1280x1000 to 1280000\n",
    "        self.fc1 = nn.Linear(1280 * 1000, 1024)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(1024, 512)          # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(512, 3)             # Output layer for 3 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input\n",
    "        x = x.view(-1, 1280 * 1000)  # Ensure input tensor is reshaped to (batch_size, 1280*1000)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation function here as it will be used with CrossEntropyLoss\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:35:42.434587900Z",
     "start_time": "2024-07-31T14:35:42.429998100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def test_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Tests the given model on the provided test data loader.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The model to test.\n",
    "        test_loader (torch.utils.data.DataLoader): DataLoader for the test set.\n",
    "        device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_mse = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            print(labels)\n",
    "            print(outputs.squeeze())\n",
    "            criterion = nn.MSELoss()\n",
    "            all_mse += criterion(outputs.squeeze(), labels)\n",
    "            print('----------------------------------------------')\n",
    "        print(all_mse)\n",
    "\n",
    "\n",
    "    accuracy = 0\n",
    "\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:35:43.742251500Z",
     "start_time": "2024-07-31T14:35:43.734089800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2014.2519\n",
      "tensor([91.3100, 83.7200, 87.5300, 84.6600], dtype=torch.float64)\n",
      "tensor([163.2999,  92.8395,  54.1345,  24.8141])\n",
      "----------------------------------------------\n",
      "tensor([79.3000, 80.7300, 80.0500, 83.9600], dtype=torch.float64)\n",
      "tensor([104.6040,  42.5186,  52.5106,  52.8992])\n",
      "----------------------------------------------\n",
      "tensor([73.8600, 68.5100, 51.3000, 87.9500], dtype=torch.float64)\n",
      "tensor([ 19.2271,  67.6752,   8.6170, 118.8757])\n",
      "----------------------------------------------\n",
      "tensor([84.2500, 75.3900, 88.5300], dtype=torch.float64)\n",
      "tensor([ 56.4988, 103.8664,  49.9369])\n",
      "----------------------------------------------\n",
      "tensor(5910.9362, dtype=torch.float64)\n",
      "Epoch 2, Loss: 1196.8935\n",
      "tensor([91.3100, 83.7200, 87.5300, 84.6600], dtype=torch.float64)\n",
      "tensor([227.1815,  89.4640,  62.5539,  35.3793])\n",
      "----------------------------------------------\n",
      "tensor([79.3000, 80.7300, 80.0500, 83.9600], dtype=torch.float64)\n",
      "tensor([135.3218,  57.3460,  99.0875,  63.4852])\n",
      "----------------------------------------------\n",
      "tensor([73.8600, 68.5100, 51.3000, 87.9500], dtype=torch.float64)\n",
      "tensor([ 30.7788,  96.5462,  14.7623, 147.5322])\n",
      "----------------------------------------------\n",
      "tensor([84.2500, 75.3900, 88.5300], dtype=torch.float64)\n",
      "tensor([ 66.2223, 135.6241,  81.7878])\n",
      "----------------------------------------------\n",
      "tensor(9717.9705, dtype=torch.float64)\n",
      "Epoch 3, Loss: 851.3006\n",
      "tensor([91.3100, 83.7200, 87.5300, 84.6600], dtype=torch.float64)\n",
      "tensor([187.1965, 101.9831,  78.1945,  48.9079])\n",
      "----------------------------------------------\n",
      "tensor([79.3000, 80.7300, 80.0500, 83.9600], dtype=torch.float64)\n",
      "tensor([121.0778,  47.6620,  99.1109,  70.6421])\n",
      "----------------------------------------------\n",
      "tensor([73.8600, 68.5100, 51.3000, 87.9500], dtype=torch.float64)\n",
      "tensor([ 31.1703,  90.3642,  20.8082, 128.1505])\n",
      "----------------------------------------------\n",
      "tensor([84.2500, 75.3900, 88.5300], dtype=torch.float64)\n",
      "tensor([ 76.8567, 135.0366,  93.0495])\n",
      "----------------------------------------------\n",
      "tensor(5990.5680, dtype=torch.float64)\n",
      "Epoch 4, Loss: 560.0652\n",
      "tensor([91.3100, 83.7200, 87.5300, 84.6600], dtype=torch.float64)\n",
      "tensor([137.8445,  71.4534,  61.3593,  48.7971])\n",
      "----------------------------------------------\n",
      "tensor([79.3000, 80.7300, 80.0500, 83.9600], dtype=torch.float64)\n",
      "tensor([99.5231, 44.1985, 66.8878, 47.9890])\n",
      "----------------------------------------------\n",
      "tensor([73.8600, 68.5100, 51.3000, 87.9500], dtype=torch.float64)\n",
      "tensor([ 35.6704,  76.6873,  24.8748, 119.7094])\n",
      "----------------------------------------------\n",
      "tensor([84.2500, 75.3900, 88.5300], dtype=torch.float64)\n",
      "tensor([66.3333, 80.2886, 60.9044])\n",
      "----------------------------------------------\n",
      "tensor(3051.8756, dtype=torch.float64)\n",
      "Epoch 5, Loss: 408.5320\n",
      "tensor([91.3100, 83.7200, 87.5300, 84.6600], dtype=torch.float64)\n",
      "tensor([141.9899,  82.5391,  75.2088,  61.5652])\n",
      "----------------------------------------------\n",
      "tensor([79.3000, 80.7300, 80.0500, 83.9600], dtype=torch.float64)\n",
      "tensor([110.7699,  49.7821,  99.3684,  60.1815])\n",
      "----------------------------------------------\n",
      "tensor([73.8600, 68.5100, 51.3000, 87.9500], dtype=torch.float64)\n",
      "tensor([ 41.2606,  89.4814,  31.6778, 143.4125])\n",
      "----------------------------------------------\n",
      "tensor([84.2500, 75.3900, 88.5300], dtype=torch.float64)\n",
      "tensor([ 88.2598, 105.1609,  79.9457])\n",
      "----------------------------------------------\n",
      "tensor(3101.7095, dtype=torch.float64)\n",
      "Epoch 6, Loss: 281.8330\n",
      "tensor([91.3100, 83.7200, 87.5300, 84.6600], dtype=torch.float64)\n",
      "tensor([129.4556,  65.3679,  66.8836,  60.3565])\n",
      "----------------------------------------------\n",
      "tensor([79.3000, 80.7300, 80.0500, 83.9600], dtype=torch.float64)\n",
      "tensor([95.5312, 44.5592, 86.4582, 47.9653])\n",
      "----------------------------------------------\n",
      "tensor([73.8600, 68.5100, 51.3000, 87.9500], dtype=torch.float64)\n",
      "tensor([ 41.3294,  66.8123,  34.8725, 127.8778])\n",
      "----------------------------------------------\n",
      "tensor([84.2500, 75.3900, 88.5300], dtype=torch.float64)\n",
      "tensor([87.6571, 82.3346, 63.4545])\n",
      "----------------------------------------------\n",
      "tensor(2390.1609, dtype=torch.float64)\n",
      "Epoch 7, Loss: 201.9801\n",
      "tensor([91.3100, 83.7200, 87.5300, 84.6600], dtype=torch.float64)\n",
      "tensor([104.9579,  72.9546,  73.6976,  69.3914])\n",
      "----------------------------------------------\n",
      "tensor([79.3000, 80.7300, 80.0500, 83.9600], dtype=torch.float64)\n",
      "tensor([103.7101,  49.7840,  95.1248,  53.5355])\n",
      "----------------------------------------------\n",
      "tensor([73.8600, 68.5100, 51.3000, 87.9500], dtype=torch.float64)\n",
      "tensor([ 54.1392,  86.5239,  42.9117, 120.7909])\n",
      "----------------------------------------------\n",
      "tensor([84.2500, 75.3900, 88.5300], dtype=torch.float64)\n",
      "tensor([69.8332, 99.7288, 68.5650])\n",
      "----------------------------------------------\n",
      "tensor(1723.4424, dtype=torch.float64)\n",
      "Epoch 8, Loss: 114.7121\n",
      "tensor([91.3100, 83.7200, 87.5300, 84.6600], dtype=torch.float64)\n",
      "tensor([116.6774,  70.9713,  70.7046,  75.1459])\n",
      "----------------------------------------------\n",
      "tensor([79.3000, 80.7300, 80.0500, 83.9600], dtype=torch.float64)\n",
      "tensor([112.7389,  48.3317,  96.7083,  56.6399])\n",
      "----------------------------------------------\n",
      "tensor([73.8600, 68.5100, 51.3000, 87.9500], dtype=torch.float64)\n",
      "tensor([ 52.6156,  86.9812,  47.8694, 131.8225])\n",
      "----------------------------------------------\n",
      "tensor([84.2500, 75.3900, 88.5300], dtype=torch.float64)\n",
      "tensor([87.0924, 84.9146, 69.7018])\n",
      "----------------------------------------------\n",
      "tensor(1926.2045, dtype=torch.float64)\n",
      "Epoch 9, Loss: 71.9178\n",
      "tensor([91.3100, 83.7200, 87.5300, 84.6600], dtype=torch.float64)\n",
      "tensor([100.5258,  72.8734,  73.5662,  77.1559])\n",
      "----------------------------------------------\n",
      "tensor([79.3000, 80.7300, 80.0500, 83.9600], dtype=torch.float64)\n",
      "tensor([108.1724,  54.5786, 101.4293,  59.4483])\n",
      "----------------------------------------------\n",
      "tensor([73.8600, 68.5100, 51.3000, 87.9500], dtype=torch.float64)\n",
      "tensor([ 58.4089,  88.4874,  52.2172, 133.1412])\n",
      "----------------------------------------------\n",
      "tensor([84.2500, 75.3900, 88.5300], dtype=torch.float64)\n",
      "tensor([80.5646, 87.2065, 66.2290])\n",
      "----------------------------------------------\n",
      "tensor(1644.4010, dtype=torch.float64)\n",
      "Epoch 10, Loss: 46.6851\n",
      "tensor([91.3100, 83.7200, 87.5300, 84.6600], dtype=torch.float64)\n",
      "tensor([113.4909,  75.6992,  76.5198,  83.3054])\n",
      "----------------------------------------------\n",
      "tensor([79.3000, 80.7300, 80.0500, 83.9600], dtype=torch.float64)\n",
      "tensor([115.5149,  54.3345, 106.2593,  61.0868])\n",
      "----------------------------------------------\n",
      "tensor([73.8600, 68.5100, 51.3000, 87.9500], dtype=torch.float64)\n",
      "tensor([ 60.6915,  92.8643,  56.0054, 137.1589])\n",
      "----------------------------------------------\n",
      "tensor([84.2500, 75.3900, 88.5300], dtype=torch.float64)\n",
      "tensor([86.0400, 85.4673, 65.6852])\n",
      "----------------------------------------------\n",
      "tensor(1985.8618, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AudioClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            data = data.to(device).float()  # Convert data to float\n",
    "            targets = targets.to(device).float()  # Convert targets to float\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)  # Add channel dimension\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')\n",
    "        accuracy = test_model(model, testdataloader, device)\n",
    "\n",
    "train(model, device, traindataloader, optimizer, criterion)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T14:38:15.048564300Z",
     "start_time": "2024-07-31T14:35:46.996449300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
